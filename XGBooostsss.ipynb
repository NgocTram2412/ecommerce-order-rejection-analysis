{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4642886,"sourceType":"datasetVersion","datasetId":2699295}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **ƒê·ªì √°n: Ph√¢n lo·∫°i v√† ƒë·ª± ƒëo√°n kh·∫£ nƒÉng ƒë∆°n h√†ng b·ªã t·ª´ ch·ªëi trong Th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠**\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"background\"></a>\n# **1. ƒê·∫∑t v·∫•n ƒë·ªÅ:**\n\n- B·ªëi c·∫£nh nghi√™n c·ª©u: Trong lƒ©nh v·ª±c th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ (E-commerce), t·ª∑ l·ªá ƒë∆°n h√†ng b·ªã t·ª´ ch·ªëi ho·∫∑c h·ªßy b·ªè (Order Rejection) l√† m·ªôt th√°ch th·ª©c l·ªõn ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp ƒë·∫øn doanh thu v√† chi ph√≠ v·∫≠n h√†nh. Vi·ªác ph√¢n t√≠ch h√†nh vi kh√°ch h√†ng li√™n quan ƒë·∫øn c√°c quy·∫øt ƒë·ªãnh n√†y l√† c∆° s·ªü quan tr·ªçng ƒë·ªÉ t·ªëi ∆∞u h√≥a quy tr√¨nh h·∫≠u c·∫ßn.\n\n- C√¢u h·ªèi nghi√™n c·ª©u: Kh·∫£ nƒÉng m·ªôt ƒë∆°n h√†ng sau khi ƒë·∫∑t mua s·∫Ω b·ªã t·ª´ ch·ªëi/h·ªßy l√† bao nhi√™u? V√† y·∫øu t·ªë n√†o g√¢y ra nh·ªØng s·ª± t·ª´ ch·ªëi/h·ªßy n√†y?\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"objectives\"></a>\n# **2.M·ª•c ti√™u nghi√™n c·ª©u:**\n\n- ·ª®ng d·ª•ng v√† so s√°nh c√°c thu·∫≠t to√°n H·ªçc m√°y (Machine Learning) ƒë·ªÉ x√¢y d·ª±ng m√¥ h√¨nh ph√¢n l·ªõp c√≥ kh·∫£ nƒÉng d·ª± ƒëo√°n ch√≠nh x√°c tr·∫°ng th√°i r·ªßi ro (B·ªã t·ª´ ch·ªëi/H·ªßy) c·ªßa ƒë∆°n h√†ng m·ªõi.\n\n- X√°c ƒë·ªãnh v√† x·∫øp h·∫°ng m·ª©c ƒë·ªô quan tr·ªçng c·ªßa c√°c ƒë·∫∑c tr∆∞ng ·∫£nh h∆∞·ªüng ƒë·∫øn k·∫øt qu·∫£ d·ª± b√°o\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"data_extraction\"></a>\n# **3. TR√çCH XU·∫§T D·ªÆ LI·ªÜU**\n<a id=\"import_libraries\"></a>\n## **3.a. Import (N·∫°p) c√°c th∆∞ vi·ªán li√™n quan**\n\n","metadata":{}},{"cell_type":"code","source":"!pip install -q scikit-learn==1.3.2\n\n# C√†i 'imbalanced-learn' b·∫£n 0.11.0 \n!pip install -q imbalanced-learn==0.11.0\n\n# C√†i 'pandas' b·∫£n 2.0.3 \n!pip install -q pandas==2.0.3","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom glob import glob\nimport warnings\n\n# Th∆∞ vi·ªán cho Ti·ªÅn x·ª≠ l√Ω v√† C√¢n b·∫±ng d·ªØ li·ªáu\nfrom imblearn.over_sampling import SMOTENC # S·ª≠ d·ª•ng SMOTE cho d·ªØ li·ªáu h·ªón h·ª£p\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler # ƒê·ªïi sang StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import ColumnTransformer\n\n# Th∆∞ vi·ªán cho M√¥ h√¨nh h√≥a v√† ƒê√°nh gi√°\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, roc_auc_score, classification_report, accuracy_score\nfrom sklearn import set_config\n\n# C·∫•u h√¨nh hi·ªÉn th·ªã\nsns.set()\nset_config(display=\"diagram\")\nwarnings.simplefilter(action=\"ignore\", category=\"FutureWarning\") # ·∫®n c√°c c·∫£nh b√°o phi√™n b·∫£n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"check_file_names\"></a>\n## **3.b. Ki·ªÉm tra t√™n file trong ƒë∆∞·ªùng d·∫´n dataset**\n\n","metadata":{}},{"cell_type":"code","source":"# S·ª≠ d·ª•ng glob ƒë·ªÉ t√¨m T·∫§T C·∫¢ file . csv trong th∆∞ m·ª•c input\n# Pattern: \"/kaggle/input/unlock-profits-with-e-commerce-sales-data/*.csv\"\n# T√¨m m·ªçi file c√≥ ƒëu√¥i .csv trong th∆∞ m·ª•c n√†y\nfilepath_list = glob(\"/kaggle/input/unlock-profits-with-e-commerce-sales-data/*.csv\")\n\n# In ra danh s√°ch c√°c file t√¨m ƒë∆∞·ª£c (ƒë·ªÉ ki·ªÉm tra)\nprint(f\"T√™n c·ªßa t·∫•t c·∫£ {len(filepath_list)} file csv trong th∆∞ m·ª•c l√†: \")\nfor number, filepath_name in enumerate(filepath_list, start=1):\n    # C·∫Øt b·ªè ph·∫ßn ƒë∆∞·ªùng d·∫´n d√†i, ch·ªâ l·∫•y t√™n file\n    # rsplit(sep='/', maxsplit=1): C·∫Øt t·ª´ ph·∫£i sang tr√°i, ch·ªâ c·∫Øt 1 l·∫ßn\n    print(f\" {number}.{filepath_name.rsplit(sep='/', maxsplit=1)[1]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"read_dataset_to_dataframe\"></a>\n## **3.c. ƒê·ªçc dataset v√†o m·ªôt dataframe pandas t·∫°m th·ªùi**\n","metadata":{}},{"cell_type":"code","source":"# ƒê·ªçc file CSV ƒë·∫ßu ti√™n (filepath_list[0]) v√†o DataFrame\n# - low_memory=False: T·∫Øt c·∫£nh b√°o khi ƒë·ªçc file l·ªõn (t·ª± ƒë·ªông detect dtype)\ndataframe = pd.read_csv(filepath_list[0], low_memory=False)\n\n# Hi·ªÉn th·ªã th√¥ng tin t·ªïng quan c·ªßa DataFrame\n# ‚Üí S·ªë d√≤ng, s·ªë c·ªôt, ki·ªÉu d·ªØ li·ªáu, s·ªë gi√° tr·ªã non-null\ndataframe.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"data_transformation_wrangle\"></a>\n# 4. DATA TRANSFORMATION**\n<a id=\"first_draft_wrangle\"></a>\n## 4.a. First Draft Wrangle Function\n- H√†m n√†y d√πng ƒë·ªÉ l√†m s·∫°ch s∆° b·ªô (preliminary cleansing) dataframe v√† ch∆∞a ph·∫£i l√† h√†m l√†m s·∫°ch cu·ªëi c√πng, s·∫Ω c·∫£i ti·ªÉn sau EDA\n\n- C√°c b∆∞·ªõc:\n\n    1. Chu·∫©n h√≥a t√™n c·ªôt (lowercase, x√≥a space, x√≥a dash)\n    2. X√≥a c√°c c·ªôt kh√¥ng c·∫ßn thi·∫øt\n    3.  ƒêi·ªÅn gi√° tr·ªã NaN cho location data\n    4. Chu·∫©n h√≥a t√™n bang (shipstate)\n    5. X√≥a d√≤ng tr√πng l·∫∑p","metadata":{}},{"cell_type":"code","source":"def first_draft_wrangle(dataframe):\n    \n # B∆Ø·ªöC 1: T·∫°o b·∫£n sao (copy) ƒë·ªÉ tr√°nh thay ƒë·ªïi dataframe g·ªëc\n    df = dataframe.copy()\n    \n # B∆Ø·ªöC 2: Chu·∫©n h√≥a t√™n c·ªôt\n    # V√≠ d·ª•: \"Order ID\" ‚Üí \"orderid\", \"ship-service-level\" ‚Üí \"shipservicelevel\"\n    col = [element.lower().replace(\" \", \"\").replace(\"-\", \"\") \n           for element in df.columns]\n    df.columns = col\n    \n# B∆Ø·ªöC 3: X√≥a c√°c c·ªôt \"r√°c\" (kh√¥ng c·∫ßn cho ph√¢n t√≠ch)\n    # - index: C·ªôt index d∆∞ th·ª´a\n    # - date: Kh√¥ng ph√¢n t√≠ch time series\n    # - fulfilledby: D∆∞ th·ª´a (c√≥ c·ªôt 'fulfilment' r·ªìi)\n    # - currency: T·∫•t c·∫£ ƒë·ªÅu INR (Indian Rupees)\n    # - unnamed:22: C·ªôt r·ªóng (data entry error)\n    # - promotionids: Qu√° nhi·ªÅu gi√° tr·ªã unique (high cardinality)\n    # - courierstatus: Kh√¥ng li√™n quan tr·ª±c ti·∫øp\n    # - shipcountry: T·∫•t c·∫£ ƒë·ªÅu India\n    df.drop([\"index\", \"date\", \"fulfilledby\", \"currency\", \"unnamed:22\",\n             \"promotionids\", \"courierstatus\", \"shipcountry\"],\n            axis=\"columns\", inplace=True)\n    \n# B∆Ø·ªöC 4: X·ª≠ l√Ω gi√° tr·ªã thi·∫øu (NaN) cho location data\n    # ƒêi·ªÅn \"unknown\" cho bang v√† th√†nh ph·ªë b·ªã thi·∫øu\n    df[\"shipstate\"].fillna(\"unknown\", inplace=True)\n    df[\"shipcity\"].fillna(\"unknown\", inplace=True)\n    \n    # ƒêi·ªÅn 0 cho postal code thi·∫øu, sau ƒë√≥ chuy·ªÉn sang object (categorical)\n    df[\"shippostalcode\"].fillna(0, inplace=True)\n    df[\"shippostalcode\"] = df[\"shippostalcode\"].astype(int).astype(object)\n    \n# B∆Ø·ªöC 5: Chu·∫©n h√≥a t√™n bang (shipstate)\n    # 5.1. Vi·∫øt hoa t·∫•t c·∫£ (ƒë·ªÉ d·ªÖ so s√°nh)\n    df[\"shipstate\"] = df[\"shipstate\"].str.upper()\n    \n    # 5.2.Gom c√°c t√™n b·ªã g√µ sai/vi·∫øt t·∫Øt v·ªÅ t√™n chu·∫©n\n    # V√≠ d·ª•: \"RJ\" ‚Üí \"RAJASTHAN\", \"DELHI\" ‚Üí \"NEW DELHI\"\n    df[\"shipstate\"].replace({\n        \"PONDICHERRY\": \"PUDUCHERRY\",\n        \"RAJSHTHAN\": \"RAJASTHAN\",    # Typo\n        \"RAJSTHAN\": \"RAJASTHAN\",      # Typo\n        \"RJ\": \"RAJASTHAN\",            # Abbreviation\n        \"PB\": \"PUNJAB\",               # Abbreviation\n        \"PUNJAB/MOHALI/ZIRAKPUR\": \"PUNJAB\",\n        \"ORISSA\": \"ODISHA\",           # Old name\n        \"DELHI\": \"NEW DELHI\",\n        \"NL\": \"UNKNOWN\",              # Invalid code\n        \"APO\": \"UNKNOWN\",             # Military address\n        \"AR\": \"UNKNOWN\"               # Invalid\n    }, inplace=True)\n    \n# B∆Ø·ªöC 6: X√≥a c√°c d√≤ng tr√πng l·∫∑p (n·∫øu c√≥)\n    df.drop_duplicates(inplace=True)\n    \n# B∆Ø·ªöC 7: Tr·∫£ v·ªÅ DataFrame ƒë√£ l√†m s·∫°ch\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"eda\"></a>\n## **4.b. Ph√¢n t√≠ch Kh√°m ph√° D·ªØ li·ªáu (EDA)**","metadata":{"execution":{"iopub.status.busy":"2023-01-22T09:37:21.810072Z","iopub.execute_input":"2023-01-22T09:37:21.810501Z","iopub.status.idle":"2023-01-22T09:37:21.817787Z","shell.execute_reply.started":"2023-01-22T09:37:21.810466Z","shell.execute_reply":"2023-01-22T09:37:21.816122Z"}}},{"cell_type":"markdown","source":"<a id=\"preliminary_data_cleaning\"></a>\n### **4.b.i. L√†m s·∫°ch d·ªØ li·ªáu s∆° b·ªô**<br>\nS·ª≠ d·ª•ng h√†m first_draft_wrangle(), ch√∫ng ta ƒë·ªçc v√† l∆∞u tr·ªØ dataset ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch s∆° b·ªô v√†o dataframe t√™n l√† \"df\".","metadata":{}},{"cell_type":"code","source":"df = first_draft_wrangle(dataframe)\n#Ki·ªÉm tra k·∫øt qu·∫£ sau khi l√†m s·∫°ch \ndf.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"orderid\"></a>\n### <br>**4.b.ii. Ph√¢n t√≠ch c·ªôt \"orderid\" (M√£ ƒë∆°n h√†ng).**<br>\n**M·ª•c ƒë√≠ch:**\n- Ki·ªÉm tra xem c√≥ bao nhi√™u ƒë∆°n h√†ng \"mua 1 s·∫£n ph·∫©m\" vs \"mua nhi·ªÅu s·∫£n ph·∫©m\"\n- Quy·∫øt ƒë·ªãnh c√≥ n√™n gi·ªØ l·∫°i ƒë∆°n h√†ng \"nhi·ªÅu s·∫£n ph·∫©m\" hay kh√¥ng","metadata":{}},{"cell_type":"code","source":"\n# T·∫†O DATAFRAME CH·ª®A C√ÅC D√íNG C√ì ORDERID L·∫∂P L·∫†I\n# - duplicated(keep=False): ƒê√°nh d·∫•u T·∫§T C·∫¢ c√°c d√≤ng c√≥ orderid l·∫∑p\n#   (keep=False ‚Üí kh√¥ng gi·ªØ d√≤ng n√†o, ƒë√°nh d·∫•u h·∫øt)\n# V√≠ d·ª•: orderid \"ABC123\" xu·∫•t hi·ªán 3 l·∫ßn ‚Üí 3 d√≤ng ƒë·ªÅu ƒë∆∞·ª£c ƒë√°nh d·∫•u True\norderid_repeat_rows = df[df[\"orderid\"].duplicated(keep=False)]\n\n# ƒê·∫æM S·ªê L∆Ø·ª¢NG ORDERID (DUY NH·∫§T) B·ªä L·∫∂P\n# nunique() = number of unique values\nunique_orderid_repeat_count = orderid_repeat_rows[\"orderid\"].nunique()\n\n# ƒê·∫æM T·ªîNG S·ªê D√íNG (ROWS) B·ªä CHI·∫æM B·ªûI C√ÅC ƒê∆†N H√ÄNG L·∫∂P\norderid_repeat_rows_count = len(orderid_repeat_rows)\n\n# ƒê·∫æM S·ªê D√íNG C·ª¶A C√ÅC ƒê∆†N H√ÄNG \"KH√îNG L·∫∂P\" (Mua 1 m√≥n)\n# duplicated(keep=False) == False: Ch·ªâ l·∫•y c√°c d√≤ng orderid KH√îNG l·∫∑p\norderid_nonrepeat_rows = df[df[\"orderid\"].duplicated(keep=False) == False]\norderid_nonrepeat_rows_count = len(orderid_nonrepeat_rows)\n\n# IN K·∫æT QU·∫¢\nprint(f\"1.  S·ªë l∆∞·ª£ng orderid (duy nh·∫•t) mua nhi·ªÅu s·∫£n ph·∫©m: {unique_orderid_repeat_count}\\n\"\n      f\"2. T·ªïng s·ªë d√≤ng c·ªßa c√°c orderid mua nhi·ªÅu s·∫£n ph·∫©m: {orderid_repeat_rows_count}\\n\"\n      f\"3. S·ªë l∆∞·ª£ng orderid (duy nh·∫•t) mua m·ªôt s·∫£n ph·∫©m: {orderid_nonrepeat_rows_count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Sau ƒë√≥ ti·∫øn h√†nh v·∫Ω bi·ªÉu ƒë·ªì c·ªôt th·ªÉ hi·ªán t·ª∑ l·ªá c·ªßa hai lo·∫°i ƒë∆°n h√†ng n√†y","metadata":{}},{"cell_type":"code","source":"# T·∫°o figure v√† axes\nfig, ax = plt.subplots(figsize=(7, 5))\n\n# T√çNH T·ª∂ L·ªÜ % c·ªßa 2 lo·∫°i ƒë∆°n h√†ng\n# - value_counts(normalize=True): ƒê·∫øm + t√≠nh t·ª∑ l·ªá\n# - *100: Chuy·ªÉn th√†nh %\n# - round(... , 2): L√†m tr√≤n 2 s·ªë l·∫ª\norderid_repeat_rows = df[df[\"orderid\"].duplicated(keep=False)]\nunique_orderid_repeat_list = orderid_repeat_rows[\"orderid\"].unique()\nplot_dataseries = round(\n    df[\"orderid\"].isin(unique_orderid_repeat_list).value_counts(normalize=True) * 100, \n    2\n)\n\n# V·∫º BI·ªÇU ƒê·ªí C·ªòT\nplot_dataseries.plot(kind=\"bar\", ax=ax)\n\n# TI√äU ƒê·ªÄ V√Ä NH√ÉN\nplt.title(\"T·ª∑ l·ªá c√°c ƒê∆°n h√†ng Mua 1 s·∫£n ph·∫©m \\nv√† Mua nhi·ªÅu s·∫£n ph·∫©m\", fontsize=16)\nplt.ylabel(\"T·ª∑ l·ªá\", fontsize=14)\nplt.xlabel(\"Lo·∫°i ƒë∆°n h√†ng\", fontsize=14)\nplt.xticks(ticks=plot_dataseries.index,\n           labels=[\"ƒê∆°n h√†ng 1 s·∫£n ph·∫©m\", \"ƒê∆°n h√†ng nhi·ªÅu s·∫£n ph·∫©m\"], \n           rotation=\"horizontal\")\nplt.yticks(ticks=[20, 40, 60, 80, 100, 120], \n           labels=[\"20%\", \"40%\", \"60%\", \"80%\", \"100%\", \"120%\"])\n\n# TH√äM NH√ÉN D·ªÆ LI·ªÜU (Data labels)\n# T·∫°o nh√£n % (v√≠ d·ª•: \"88. 03%\")\ndata_label = plot_dataseries.astype(str).str.cat(np.full((2,), \"%\"), sep=\"\")\n# T·∫°o nh√£n s·ªë ƒë·∫øm (v√≠ d·ª•: \"35000\")\ncount_label = pd.Series(df[\"orderid\"].isin(unique_orderid_repeat_list).value_counts()).astype(\"str\")\n\n# V·∫Ω nh√£n % l√™n bi·ªÉu ƒë·ªì\nfor x, y in enumerate(plot_dataseries):\n    plt.text(x, y-10, data_label[x], color=\"white\",\n             fontweight=700, fontsize=14, horizontalalignment=\"center\")\n\n# V·∫Ω nh√£n s·ªë ƒë·∫øm l√™n bi·ªÉu ƒë·ªì\nfor x, y in enumerate(plot_dataseries):\n    plt.text(x, y+5, (\"S·ªë l∆∞·ª£ng:\\n\" + count_label[x]), color=\"Darkgreen\",\n             fontweight=700, fontsize=14, horizontalalignment=\"center\")\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### NH·∫¨N X√âT:\n\n**Quan s√°t:**\n- 88.04% (g·∫ßn 90%) d·ªØ li·ªáu l√† t·ª´ c√°c ƒë∆°n h√†ng \"mua 1 s·∫£n ph·∫©m\" (single-product order)\n- Ch·ªâ c√≥ 11.96% l√† ƒë∆°n h√†ng \"mua nhi·ªÅu s·∫£n ph·∫©m\" (multi-product order)\n\n**L√Ω do:**\n- \"Multi-product orders introduce complexity: \n  - C√≥ th·ªÉ 1 s·∫£n ph·∫©m b·ªã reject, 1 s·∫£n ph·∫©m OK\n  - Kh√≥ x√°c ƒë·ªãnh target r√µ r√†ng (rejected hay kh√¥ng?)\n  - Analysis ri√™ng bi·ªát cho multi-product orders recommended\"\n\n**Quy·∫øt ƒë·ªãnh:**\n**X√≥a c√°c ƒë∆°n h√†ng \"mua nhi·ªÅu s·∫£n ph·∫©m\"**\n- L√Ω do 1: Chi·∫øm t·ª∑ l·ªá nh·ªè (ch·ªâ 12%)\n- L√Ω do 2: Kh√≥ x√°c ƒë·ªãnh target (rejected hay kh√¥ng unclear)\n- L√Ω do 3: Focus v√†o ƒëa s·ªë (88%) ƒë·ªÉ c√≥ k·∫øt qu·∫£ r√µ r√†ng h∆°n\n\n**Sau ƒë√≥ x√≥a c·ªôt \"orderid\"** V√¨ sau khi ch·ªâ gi·ªØ \"ƒë∆°n 1 m√≥n\" ‚Üí orderid kh√¥ng c√≤n c·∫ßn thi·∫øt (m·ªói d√≤ng = 1 ƒë∆°n ri√™ng bi·ªát)","metadata":{"execution":{"iopub.status.busy":"2023-01-22T09:52:29.486675Z","iopub.execute_input":"2023-01-22T09:52:29.487094Z","iopub.status.idle":"2023-01-22T09:52:29.495721Z","shell.execute_reply.started":"2023-01-22T09:52:29.48706Z","shell.execute_reply":"2023-01-22T09:52:29.494276Z"}}},{"cell_type":"code","source":"# Lo·∫°i b·ªè c√°c ƒë∆°n h√†ng c√≥ ch·ª©a nhi·ªÅu s·∫£n ph·∫©m (ch·ªâ gi·ªØ l·∫°i ƒë∆°n h√†ng mua 1 s·∫£n ph·∫©m duy nh·∫•t)\ndf = df[df[\"orderid\"].duplicated(keep = False) == False]\n# drop redundant column \"orderid\"\n# X√≥a c·ªôt \"orderid\" v√¨ n√≥ ƒë√£ tr·ªü n√™n d∆∞ th·ª´a \ndf.drop(\"orderid\", axis = 1, inplace = True)\n# Xem l·∫°i th√¥ng tin dataframe sau khi x·ª≠ l√Ω\ndf.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **4.b.iii. T·∫°o c·ªôt \"rejected\" (Target Variable - Bi·∫øn m·ª•c ti√™u)**\n\n**M·ª•c ƒë√≠ch:**\n- T·∫°o bi·∫øn m·ª•c ti√™u nh·ªã ph√¢n (binary target) cho b√†i to√°n classification\n- Chuy·ªÉn t·ª´ c·ªôt \"status\" (c√≥ nhi·ªÅu gi√° tr·ªã) ‚Üí c·ªôt \"rejected\" (ch·ªâ c√≥ 0/1)\n- Target Definition:\n  - **1 (Rejected):** C√°c ƒë∆°n h√†ng b·ªã h·ªßy/tr·∫£ l·∫°i\n  - **0 (Not Rejected):** C√°c ƒë∆°n h√†ng giao th√†nh c√¥ng\n\n**Ph∆∞∆°ng ph√°p: Binary Encoding**\n- **B∆∞·ªõc 1:** X√°c ƒë·ªãnh c√°c gi√° tr·ªã \"status\" r√µ r√†ng (confirmed outcomes)\n- **B∆∞·ªõc 2:** Lo·∫°i b·ªè c√°c status \"l∆° l·ª≠ng\" (Pending, Shipped without delivery confirmation)\n- **B∆∞·ªõc 3:** Encode: Cancelled', 'Shipped - Returned to Seller', 'Shipped - Rejected by Buyer', 'Shipped - Returning to Seller' ‚Üí 1, 'Shipped - Delivered to Buyer' ‚Üí 0\n\n**L√Ω do lo·∫°i b·ªè status \"l∆° l·ª≠ng\":**\n- \"Pending\": Ch∆∞a bi·∫øt k·∫øt qu·∫£ cu·ªëi c√πng\n- \"Shipped\": C√≥ th·ªÉ delivered, c√≥ th·ªÉ returned ‚Üí kh√¥ng ch·∫Øc ch·∫Øn\n- ‚Üí Gi·ªØ l·∫°i s·∫Ω g√¢y **label noise** (nhi·ªÖu nh√£n) ‚Üí gi·∫£m accuracy c·ªßa model","metadata":{}},{"cell_type":"code","source":"# L·∫•y t·∫•t c·∫£ gi√° tr·ªã unique (duy nh·∫•t) trong c·ªôt \"status\"\n# ‚Üí ƒê·ªÉ x√°c ƒë·ªãnh c√°ch ph√¢n lo·∫°i \"Rejected\" vs \"Not Rejected\"\nprint(\"C√°c gi√° tr·ªã unique trong c·ªôt 'status':\")\nprint(df[\"status\"].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X√≥a c√°c d√≤ng c√≥ status kh√¥ng r√µ r√†ng\n# 1. ƒê·ªãnh nghƒ©a c√°c status c√≥ k·∫øt qu·∫£ cu·ªëi c√πng r√µ r√†ng\nknown_value = [\"Cancelled\", 'Shipped - Returned to Seller','Shipped - Rejected by Buyer',\n            'Shipped - Returning to Seller','Shipped - Delivered to Buyer']\n# 2. L·ªçc dataframe, ch·ªâ gi·ªØ l·∫°i c√°c d√≤ng c√≥ status n·∫±m trong danh s√°ch known_value\ndf = df[df[\"status\"].isin(known_value)]    \n\n# 3. T·∫†O C·ªòT \"rejected\" (Binary: 1=Rejected, 0=Not Rejected)\nrejected = [\"Cancelled\", 'Shipped - Returned to Seller','Shipped - Rejected by Buyer',\n            'Shipped - Returning to Seller']\n# 4. N·∫øu status n·∫±m trong danh s√°ch \"rejected\" -> g√°n gi√° tr·ªã 1, ng∆∞·ª£c l·∫°i l√† 0\ndf[\"rejected\"] = df[\"status\"].isin(rejected).astype(int)    # ƒë·ªïi ki·ªÉu d·ªØ li·ªáu sang s·ªë nguy√™n (int) \n\n# 5. X√≥a (drop) c·ªôt \"status\" g·ªëc ƒëi (v√¨ ƒë√£ c√≥ c·ªôt \"rejected\" m·ªõi)\ndf.drop(\"status\",axis = \"columns\", inplace = True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<br>V·∫Ω bi·ªÉu ƒë·ªì c·ªôt bi·ªÉu di·ªÖn t·ª∑ l·ªá c·ªßa 2 status n√†y","metadata":{"execution":{"iopub.status.busy":"2023-01-22T09:57:32.762148Z","iopub.execute_input":"2023-01-22T09:57:32.762587Z","iopub.status.idle":"2023-01-22T09:57:32.769814Z","shell.execute_reply.started":"2023-01-22T09:57:32.762549Z","shell.execute_reply":"2023-01-22T09:57:32.768063Z"}}},{"cell_type":"code","source":"# v·∫Ω bi·ªÉu ƒë·ªì c·ªôt d√πng th∆∞ vi·ªán matplotlib\nfig,ax = plt.subplots(figsize = (7,5))\n\n# t√≠nh to√°n v√† l∆∞u tr·ªØ t·ª∑ l·ªá % c·ªßa 2 l·ªõp (0 v√† 1) v√†o m·ªôt pandas.Series\nplot_dataseries = round(df[\"rejected\"].value_counts(normalize = True)*100,2)\n\n# v·∫Ω bi·ªÉu ƒë·ªì c·ªôt\nplot_dataseries.plot(kind = \"bar\",ax =ax)\n# ƒê·∫∑t Ti√™u ƒë·ªÅ v√† Nh√£n \nplt.title(\"T·ª∑ l·ªá c√°c ƒë∆°n h√†ng B·ªã t·ª´ ch·ªëi v√† Kh√¥ng b·ªã t·ª´ ch·ªëi\", fontsize = 14)\nplt.ylabel(\"T·ª∑ l·ªá (%)\", fontsize = 12)\nplt.xlabel(\"Tr·∫°ng th√°i\", fontsize = 12)\nplt.xticks(ticks = range(len(plot_dataseries)),\n           labels = [\"Kh√¥ng b·ªã t·ª´ ch·ªëi\", \"B·ªã t·ª´ ch·ªëi\"], rotation = \"horizontal\") \nplt.yticks(ticks = [20,40,60,80,100], labels = [\"20%\",\"40%\",\"60%\",\"80%\",\"100%\"])\n\n# t·∫°o m·ªôt series kh√°c ƒë·ªÉ hi·ªÉn th·ªã nh√£n % (v√≠ d·ª•: \"58.2%\")\ndata_label = plot_dataseries.astype(str).str.cat(np.full((2,),\"%\"), sep = \"\")\n\n# t·∫°o th√™m m·ªôt series n·ªØa ƒë·ªÉ hi·ªÉn th·ªã s·ªë ƒë·∫øm (v√≠ d·ª•: 20000)\ndata_count = df[\"rejected\"].value_counts()\n\n# th√™m/v·∫Ω nh√£n % l√™n bi·ªÉu ƒë·ªì\nfor x,y in enumerate(plot_dataseries):\n    plt.text(x,y-10,data_label[x],color = \"white\",\n             fontweight = 700,fontsize = 13, horizontalalignment = \"center\")\n\n# th√™m/v·∫Ω nh√£n s·ªë ƒë·∫øm (Count) l√™n bi·ªÉu ƒë·ªì\nfor x,y in enumerate(plot_dataseries):\n    plt.text(x,y+5,\"S·ªë l∆∞·ª£ng:\\n\" + str(data_count[x]),fontweight = 700,\n             fontsize = 13,horizontalalignment = \"center\")\n    \nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### NH·∫¨N X√âT V·ªÄ PH√ÇN PH·ªêI TARGET:\n\n**Quan s√°t:**\n- **57.7%** l√† \"Not Rejected\" (0)\n- **42.3%** l√† \"Rejected\" (1)\n- T·ª∑ l·ªá: **58:42** (kh√¥ng qu√° m·∫•t c√¢n b·∫±ng)\n  \n**Ph√¢n t√≠ch:**\n**∆Øu ƒëi·ªÉm c·ªßa t·ª∑ l·ªá hi·ªán t·∫°i (58:42):**\n- Kh√¥ng qu√° m·∫•t c√¢n b·∫±ng (slight imbalance)\n- Class thi·ªÉu s·ªë (42%) v·∫´n c√≥ ƒë·ªß samples ƒë·ªÉ h·ªçc\n- Model c√≥ th·ªÉ h·ªçc ƒë∆∞·ª£c patterns c·ªßa c·∫£ 2 classes\n\n**Nh∆∞·ª£c ƒëi·ªÉm:**\n- Model c√≥ th·ªÉ h∆°i \"thi√™n v·ªã\" (biased) v·ªÅ class 0\n- Metrics nh∆∞ Accuracy c√≥ th·ªÉ misleading (model ƒëo√°n to√†n 0 v·∫´n ƒë∆∞·ª£c 58% accuracy)\n\n**Quy·∫øt ƒë·ªãnh:**\n\n**KH√îNG x·ª≠ l√Ω ngay ·ªü ƒë√¢y v√¨:**\n- Data hi·ªán t·∫°i l√† **TO√ÄN B·ªò dataset** (ch∆∞a chia train/test)\n- N·∫øu oversample b√¢y gi·ªù ‚Üí s·∫Ω c√≥ data leakage:\n  - Train v√† Test ƒë·ªÅu c√≥ synthetic samples\n  - Model s·∫Ω \"gian l·∫≠n\" (th·∫•y test data trong qu√° tr√¨nh train)\n  - Accuracy gi·∫£ t·∫°o (inflated accuracy)\n\n**S·∫º x·ª≠ l√Ω ·ªü B∆∞·ªõc 6. b (sau khi chia train/test):**\n1. Chia dataset ‚Üí Train/Test\n2. CH·ªà oversample (SMOTE) tr√™n TRAIN set\n3. Gi·ªØ nguy√™n Test set (kh√¥ng ƒë·ªông v√†o)\n4. ƒê·∫£m b·∫£o ƒë√°nh gi√° c√¥ng b·∫±ng","metadata":{"execution":{"iopub.status.busy":"2023-01-22T10:00:14.726914Z","iopub.execute_input":"2023-01-22T10:00:14.727997Z","iopub.status.idle":"2023-01-22T10:00:14.735397Z","shell.execute_reply.started":"2023-01-22T10:00:14.727959Z","shell.execute_reply":"2023-01-22T10:00:14.73387Z"}}},{"cell_type":"markdown","source":"### **4.b.iv.  Ki·ªÉm tra High Cardinality Features**\n**M·ª•c ƒë√≠ch:**\n- X√°c ƒë·ªãnh c√°c c·ªôt categorical c√≥ **qu√° nhi·ªÅu** gi√° tr·ªã unique (high cardinality)\n- Quy·∫øt ƒë·ªãnh xem c√≥ n√™n gi·ªØ l·∫°i hay lo·∫°i b·ªè\n\n**Ph∆∞∆°ng ph√°p: Cardinality Check**\n- ƒê·∫øm s·ªë unique values cho m·ªói c·ªôt categorical\n- Identify c√°c c·ªôt c√≥ cardinality qu√° cao (> 100)","metadata":{}},{"cell_type":"code","source":"# \"Cardinality > 50 ƒë∆∞·ª£c coi l√† cao, > 500 l√† r·∫•t cao\"\nprint(\"Ti·∫øn h√†nh ki·ªÉm tra s·ªë l∆∞·ª£ng gi√° tr·ªã duy nh·∫•t (Cardinality) c·ªßa c√°c ƒë·∫∑c tr∆∞ng ph√¢n lo·∫°i...\")\nprint()\n#  ƒê·∫øm s·ªë unique values cho M·ªñI c·ªôt c√≥ dtype='object' (categorical)\n# .select_dtypes(include=\"object\"): Ch·ªçn c√°c c·ªôt bi·∫øn ƒë·ªãnh danh/ph√¢n lo·∫°i\n# .nunique(): ƒê·∫øm s·ªë gi√° tr·ªã duy nh·∫•t cho m·ªói c·ªôt\ncardinality_series = df.select_dtypes(include=\"object\").nunique()\n\n# S·∫Øp x·∫øp t·ª´ cao xu·ªëng th·∫•p ƒë·ªÉ thu·∫≠n ti·ªán quan s√°t\ncardinality_series = cardinality_series.sort_values(ascending=False)\n\nprint(\"Th·ªëng k√™ s·ªë l∆∞·ª£ng gi√° tr·ªã duy nh·∫•t c·ªßa t·ª´ng ƒë·∫∑c tr∆∞ng:\")\nprint(cardinality_series)\n\n# X√°c ƒë·ªãnh c√°c c·ªôt c√≥ Cardinality R·∫§T CAO (> 100)\nhigh_cardinality_cols = cardinality_series[cardinality_series > 100]\n\nif len(high_cardinality_cols) > 0:\n    print(\"\\n C√ÅC ƒê·∫∂C TR∆ØNG C√ì Cardinality cao (> 100 gi√° tr·ªã duy nh·∫•t):\")\n    for col, count in high_cardinality_cols.items():\n        print(f\"   ‚Ä¢ ƒê·∫∑c tr∆∞ng '{col}': {count} gi√° tr·ªã duy nh·∫•t\")\nelse:\n    print(\"\\n Kh√¥ng ph√°t hi·ªán ƒë·∫∑c tr∆∞ng n√†o c√≥ Cardinality b·∫•t th∆∞·ªùng (High Cardinality).\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### NH·∫¨N X√âT & QUY·∫æT ƒê·ªäNH:\n\n**Quan s√°t:**\n\nC√°c c·ªôt sau c√≥ **High Cardinality** (qu√° nhi·ªÅu unique values):`sku`, `style`, `shipcity`, `shippostalcode`, `asin`. -->X√≥a\n\n**Gi·ªØ l·∫°i c√°c features c√≥ cardinality h·ª£p l√Ω:**\n- `category` (~5 values) \n- `fulfilment` (2 values) \n- `shipservicelevel` (2 values) \n- `b2b` (2 values) \n- `region` (s·∫Ω t·∫°o - 6 values) \n\n**K·∫øt qu·∫£:**\n- Gi·∫£m complexity\n- Tr√°nh overfitting\n- Faster training\n- Better generalization","metadata":{}},{"cell_type":"code","source":"df.drop([\"style\",\"sku\",\"shipcity\",\"shippostalcode\",\"asin\"],axis = 1, inplace = True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **4. b.v. Ki·ªÉm tra c√°c c·ªôt kh√¥ng ph·∫£i Categorical (Numerical + Others)**\n\n**M·ª•c ƒë√≠ch:**\n- X√°c ƒë·ªãnh c√°c c·ªôt c√≥ dtype kh√°c \"object\" (kh√¥ng ph·∫£i categorical)\n- Ki·ªÉm tra: Numerical features (s·ªë), Boolean features (True/False), etc.\n\n**Ph∆∞∆°ng ph√°p: Dtype Inspection**\n- S·ª≠ d·ª•ng `. select_dtypes(exclude=\"object\")` ƒë·ªÉ l·ªçc\n- Ki·ªÉm tra: dtype, s·ªë NaN, range values","metadata":{}},{"cell_type":"code","source":"#  hi·ªÉn th·ªã th√¥ng tin c√°c c·ªôt KH√îNG ph·∫£i l√† 'object' (t·ª©c l√† c·ªôt s·ªë, c·ªôt boolean)\ndf.select_dtypes(exclude = \"object\").info()\ndf.select_dtypes(exclude=\"object\").describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"qty\"></a>\n### <br>**4.b.vi.Ph√¢n t√≠ch c·ªôt \"qty\" (S·ªë l∆∞·ª£ng m·ªói ƒë∆°n)**\n**M·ª•c ƒë√≠ch:**\n- Ki·ªÉm tra ph√¢n ph·ªëi c·ªßa s·ªë l∆∞·ª£ng s·∫£n ph·∫©m trong m·ªói ƒë∆°n h√†ng\n- X√°c ƒë·ªãnh c√≥ b·∫•t th∆∞·ªùng (anomalies) kh√¥ng: qty=0 l√† kh√¥ng h·ª£p l·ªá\n\n**Ph∆∞∆°ng ph√°p: Value Distribution Analysis**\n- ƒê·∫øm frequency c·ªßa t·ª´ng gi√° tr·ªã qty (0, 1, 2, 3,...)\n- T√≠nh t·ª∑ l·ªá % (proportion)\n- Visualize b·∫±ng bar chart","metadata":{}},{"cell_type":"code","source":"# --- ƒêo·∫°n n√†y ƒë·ªÉ t·∫°o 1 c√°i b·∫£ng th·ªëng k√™ c·ªôt \"qty\" ---\n\n# 1. t√≠nh t·ª∑ l·ªá % c·ªßa c√°c gi√° tr·ªã trong c·ªôt \"qty\" (v√≠ d·ª•: s·ªë 0 chi·∫øm 25.83%)\nqty_prop = round(df[\"qty\"].value_counts(normalize = True)*100,2)\n\n# 2. ƒë·ªïi t√™n series, s·∫Øp x·∫øp (sort) theo index (0, 1, 2...) v√† chuy·ªÉn th√†nh dataframe\nqty_prop = qty_prop.rename(\"Proportion(%)\").sort_index().to_frame()\n\n# 3. ƒë·ªïi t√™n index (ch·ªâ m·ª•c) th√†nh \"qty_value\"\nqty_prop.index.name = \"qty_value\"\n\n# 4. th√™m c·ªôt \"Count\" (S·ªë ƒë·∫øm)\nqty_prop[\"Count\"] = df[\"qty\"].value_counts()\n\n# 5. ƒë·ªïi ki·ªÉu d·ªØ li·ªáu c·ªßa c·ªôt \"Proportion(%)\" sang chu·ªói (str) v√† th√™m d·∫•u \"%\" v√†o\nqty_prop[\"Proportion(%)\"] = (\n    qty_prop[\"Proportion(%)\"].astype(str)\n    .str.cat(np.full(fill_value = \"%\",shape = (len(qty_prop),)))\n)\n\n# In c√°i b·∫£ng th·ªëng k√™ ƒë√≥ ra\nqty_prop","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================\n# V·∫º BI·ªÇU ƒê·ªí PH√ÇN PH·ªêI C·ªòT \"QTY\"\n# ==============================================================\n\n# T·∫°o figure\nfig, ax = plt.subplots(figsize=(10, 5))\n\n# V·∫Ω bar chart (tr·ª•c Y = Count)\nqty_prop.plot(kind=\"bar\", y=\"Count\", ax=ax)\n\n# Ô∏è Ti√™u ƒë·ªÅ v√† nh√£n\nplt.xlabel(\"S·ªë l∆∞·ª£ng (Qty) m·ªói ƒë∆°n\", fontsize=13)\nplt.ylabel(\"T·∫ßn su·∫•t [S·ªë ƒë·∫øm]\", fontsize=13)\nplt.yticks(ticks=range(20000, 60001, 20000))\nplt.title('Ph√¢n ph·ªëi c·ªßa c·ªôt \"qty\"', fontsize=14, fontweight=550)\nplt.xticks(rotation=\"horizontal\")\nplt.legend(\"\")  # T·∫Øt legend\n\n#  Th√™m nh√£n % l√™n m·ªói c·ªôt\nfor x, y in enumerate(qty_prop[\"Count\"]):\n    qty_value = qty_prop.index[x]\n    plt.text(x, y+1500, qty_prop[\"Proportion(%)\"].loc[qty_value],\n             horizontalalignment=\"center\", fontweight=700)\n\n# Th√™m GHI CH√ö ƒê·∫∂C BI·ªÜT cho qty=0\nplt.text(0, 22000, \n         ' Gi√° tr·ªã n√†y\\n ƒë∆∞·ª£c coi l√†\\n \"NaN\" (L·ªói)\\n v√¨ S·ªë l∆∞·ª£ng\\n kh√¥ng th·ªÉ \\n l√† \"0\"\\n‚Üì',\n         horizontalalignment=\"center\", color=\"darkred\", fontsize=13)\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **NH·∫¨N X√âT V·ªÄ C·ªòT \"QTY\":**\n\n**Quan s√°t t·ª´ bi·ªÉu ƒë·ªì:**\n\nPh√¢n ph·ªëi c·ªôt qty cho th·∫•y 73.3% ƒë∆°n h√†ng c√≥ qty=1, trong khi 26.46% c√≥ qty=0 (b·∫•t th∆∞·ªùng v√¨ ƒë∆°n h√†ng kh√¥ng th·ªÉ c√≥ s·ªë l∆∞·ª£ng b·∫±ng 0), v√† ch·ªâ c√≥ 0.24% c√≥ qty ‚â• 2.  Gi√° tr·ªã qty=0 c√≥ th·ªÉ do data entry error, system glitch, ho·∫∑c ƒë∆°n h√†ng b·ªã cancelled tr∆∞·ªõc khi x√°c nh·∫≠n nh∆∞ng v·∫´n ƒë∆∞·ª£c ghi log v√†o h·ªá th·ªëng.\n\n**V·∫•n ƒë·ªÅ ph√°t hi·ªán:**\n- C·ªôt `qty` ch·ª©a gi√° tr·ªã 0 (ƒë∆°n h√†ng b·∫•t th∆∞·ªùng)\n- Sau ph√¢n t√≠ch, ph√°t hi·ªán `qty=0` c√≥ correlation qu√° cao v·ªõi target (potential perfect predictor)\n- **Quy·∫øt ƒë·ªãnh:** X√≥a to√†n b·ªô c·ªôt `qty` ƒë·ªÉ tr√°nh model ph·ª• thu·ªôc v√†o anomaly data\n\n**L√Ω do lo·∫°i b·ªè:**\n1. **Tr√°nh overfitting:** Feature c√≥ correlation qu√° cao (>0.5) v·ªõi target c√≥ th·ªÉ g√¢y overfitting\n2. **Generalization:** Model kh√¥ng n√™n ph·ª• thu·ªôc v√†o edge cases (qty=0)\n3. **Business logic:** Trong th·ª±c t·∫ø, c√°c ƒë∆°n h√†ng qty=0 th∆∞·ªùng ƒë∆∞·ª£c filter s·ªõm h∆°n trong pipeline\n\n**Ph∆∞∆°ng ph√°p:**\n- X√≥a tr·ª±c ti·∫øp c·ªôt `qty` kh·ªèi DataFrame\n- Kh√¥ng t·∫°o feature m·ªõi t·ª´ qty\n","metadata":{}},{"cell_type":"code","source":"# ==============================================================\n# X·ª¨ L√ù C·ªòT 'qty' (S·ªë l∆∞·ª£ng)\n# ==============================================================\n\nprint(\"=\"*70)\nprint(\"X·ª¨ L√ù C·ªòT 'qty'\")\nprint(\"=\"*70)\n\n# Ph√¢n t√≠ch: qty c√≥ gi√° tr·ªã 0 (b·∫•t th∆∞·ªùng)\nprint(\"\\nKi·ªÉm tra ph√¢n ph·ªëi qty:\")\nprint(df['qty'].value_counts().head())\nprint(f\"\\nS·ªë l∆∞·ª£ng qty=0: {(df['qty'] == 0).sum()} ({(df['qty'] == 0).sum() / len(df) * 100:.2f}%)\")\n\n# Quy·∫øt ƒë·ªãnh: X√≥a c·ªôt qty\n# L√Ω do: \n# 1. qty=0 l√† edge case, c√≥ th·ªÉ g√¢y overfitting\n# 2.  Correlation v·ªõi target qu√° cao (potential perfect predictor)\n# 3. Model n√™n h·ªçc t·ª´ features \"b√¨nh th∆∞·ªùng\" h∆°n\n\nprint(\"\\nƒêang x√≥a c·ªôt 'qty'...\")\ndf. drop(\"qty\", axis=\"columns\", inplace=True)\nprint(\" ƒê√£ x√≥a c·ªôt 'qty'\")\n\nprint(f\"\\nDataFrame sau khi x·ª≠ l√Ω:\")\nprint(f\"  - S·ªë d√≤ng: {len(df)}\")\nprint(f\"  - S·ªë c·ªôt: {len(df.columns)}\")\n\nprint(\"\\n Ho√†n th√†nh x·ª≠ l√Ω qty!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"amount\"></a>\n### <br>**4.b.vii. Ph√¢n t√≠ch c·ªôt \"amount\" (S·ªë ti·ªÅn).**\n\n**M·ª•c ƒë√≠ch:** \n- Ki·ªÉm tra ph√¢n ph·ªëi c·ªßa s·ªë ti·ªÅn ƒë∆°n h√†ng\n- X√°c ƒë·ªãnh v√† x·ª≠ l√Ω outliers (gi√° tr·ªã ngo·∫°i l·ªá)\n\n**Ph∆∞∆°ng ph√°p: Log Transformation**\n\nS·ª≠ d·ª•ng Log Transformation ƒë·ªÉ chu·∫©n h√≥a ph√¢n ph·ªëi thay v√¨ lo·∫°i b·ªè outliers. Ph∆∞∆°ng ph√°p n√†y cho ph√©p gi·ªØ nguy√™n 100% d·ªØ li·ªáu trong khi v·∫´n gi·∫£m skewness.  C√°c gi√° tr·ªã amount cao (5000 Rs) l√† ƒë∆°n h√†ng ƒë·∫Øt ti·ªÅn h·ª£p l·ªá (saree sets, premium products), kh√¥ng ph·∫£i noise hay errors, n√™n c·∫ßn preserve ƒë·ªÉ model c√≥ th·ªÉ h·ªçc ƒë∆∞·ª£c patterns ·ªü m·ªçi price ranges.","metadata":{"execution":{"iopub.status.busy":"2023-01-22T10:13:57.739187Z","iopub.execute_input":"2023-01-22T10:13:57.739612Z","iopub.status.idle":"2023-01-22T10:13:57.747968Z","shell.execute_reply.started":"2023-01-22T10:13:57.739578Z","shell.execute_reply":"2023-01-22T10:13:57.746553Z"}}},{"cell_type":"code","source":"# ==============================================================\n# KI·ªÇM TRA GI√Å TR·ªä amount = 0\n# ==============================================================\n\n#  ƒê·∫øm s·ªë d√≤ng c√≥ amount = 0\namount0_count = (df[\"amount\"] == 0).sum()\n\nprint(f\"üîç Ki·ªÉm tra gi√° tr·ªã amount = 0...\")\nprint(f\"   ‚Üí T√¨m th·∫•y {amount0_count} d√≤ng c√≥ amount = 0\")\nprint(f\"   ‚Üí T·ª∑ l·ªá: {amount0_count/len(df)*100:.2f}%\")\n\n# Gi·∫£i th√≠ch: ƒê∆°n h√†ng kh√¥ng th·ªÉ c√≥ amount = 0\n# ‚Üí ƒê√¢y l√† missing data ho·∫∑c l·ªói nh·∫≠p li·ªáu (data entry error)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================\n# X·ª¨ L√ù amount = 0 - X√ìA D√íNG (Kh√¥ng t·∫°o NaN)\n# ==============================================================\n\nprint(\"\\n X·ª≠ l√Ω c√°c d√≤ng c√≥ amount = 0...\")\n\n# ƒê·∫øm tr∆∞·ªõc khi x√≥a\nbefore_drop = len(df)\n\n# X√≥a c√°c d√≤ng c√≥ amount = 0\ndf = df[df[\"amount\"] != 0]\n\n# ƒê·∫øm sau khi x√≥a\nafter_drop = len(df)\ndropped = before_drop - after_drop\n\nprint(f\"   ‚Üí ƒê√£ x√≥a {dropped} d√≤ng c√≥ amount = 0\")\nprint(f\"   ‚Üí C√≤n l·∫°i: {after_drop} d√≤ng\")\n\n# Ki·ªÉm tra: Kh√¥ng c√≤n NaN\nprint(f\" Kh√¥ng c√≥ gi√° tr·ªã NaN (NaN count: {df['amount'].isna().sum()})\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<br>**Ki·ªÉm tra Ph√¢n ph·ªëi c·ªßa c·ªôt \"amount\"**\n","metadata":{}},{"cell_type":"code","source":"# ==============================================================\n# V·∫º BI·ªÇU ƒê·ªí PH√ÇN PH·ªêI \"AMOUNT\" (Tr∆∞·ªõc khi √°p d·ª•ng Log)\n# ==============================================================\n\n#  T·∫°o 2 bi·ªÉu ƒë·ªì: Boxplot + Histogram\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(7, 8))\n\n#  Ti√™u ƒë·ªÅ chung\nplt.suptitle(\"\\nPh√¢n ph·ªëi c·ªßa c·ªôt Amount (S·ªë ti·ªÅn) - D·ªØ li·ªáu g·ªëc\", fontweight=600)\n\n#  Bi·ªÉu ƒë·ªì 1: BOXPLOT (Bi·ªÉu ƒë·ªì h·ªôp)\ndf[\"amount\"].plot(kind=\"box\", vert=False, ax=ax1)\nax1.text(3000, 1.15, \"<-- Gi√° tr·ªã Ngo·∫°i l·ªá (Outliers) -->\", \n         fontsize=17, horizontalalignment=\"center\", color=\"darkred\")\nax1.set_ylabel('C·ªôt \"Amount\"', fontsize=12, fontweight=600)\nax1.set_yticklabels(labels=\"\")\n\n#  Bi·ªÉu ƒë·ªì 2: HISTOGRAM (Bi·ªÉu ƒë·ªì t·∫ßn su·∫•t)\ndf[\"amount\"].plot(kind=\"hist\", bins=20, ax=ax2)\nax2.text(3000, 5000, \"<-- Gi√° tr·ªã Ngo·∫°i l·ªá (Outliers) -->\", \n         fontsize=17, horizontalalignment=\"center\", color=\"darkred\")\nax2.set_ylabel('T·∫ßn su·∫•t [S·ªë ƒë·∫øm]', fontsize=12, fontweight=600)\nax2.set_xlabel(\"S·ªë ti·ªÅn (Rs.)\", fontsize=12, fontweight=600)\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### NH·∫¨N X√âT:\n\n**Quan s√°t t·ª´ bi·ªÉu ƒë·ªì:**\n\nPh√¢n ph·ªëi amount r·∫•t l·ªách ph·∫£i (right-skewed) v·ªõi nhi·ªÅu outliers ·ªü gi√° tr·ªã cao (>2000 Rs).  Ph·∫ßn l·ªõn ƒë∆°n h√†ng t·∫≠p trung ·ªü kho·∫£ng 200-800 Rs, trong khi c√≥ m·ªôt s·ªë ƒë∆°n h√†ng ƒë·∫Øt ti·ªÅn (expensive orders) l√™n ƒë·∫øn 5000-10000 Rs.\n\n**Quy·∫øt ƒë·ªãnh - √Åp d·ª•ng Log Transformation:**\n\nCh√∫ng ta s·ª≠ d·ª•ng Log Transformation (`log(amount + 1)`) thay v√¨ lo·∫°i b·ªè outliers v√¨ c√°c l√Ω do sau:\n\n**L√Ω do 1 - Preserve Information:** C√°c gi√° tr·ªã amount cao (5000 Rs) l√† ƒë∆°n h√†ng ƒë·∫Øt ti·ªÅn h·ª£p l·ªá (saree sets, premium products), kh√¥ng ph·∫£i errors.   N·∫øu lo·∫°i b·ªè ch√∫ng, model s·∫Ω m·∫•t th√¥ng tin quan tr·ªçng v·ªÅ vi·ªác \"high-value orders c√≥ rejection rate kh√°c th·∫ø n√†o so v·ªõi low-value orders\".\n\n**L√Ω do 2 - Normalize Distribution:** Log transformation gi·∫£m skewness, l√†m cho distribution g·∫ßn v·ªõi Normal distribution h∆°n, gi√∫p model h·ªçc t·ªët h∆°n (ƒë·∫∑c bi·ªát v·ªõi c√°c thu·∫≠t to√°n gi·∫£ ƒë·ªãnh Normal distribution nh∆∞ Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"<br>**Ki·ªÉm tra ph√¢n ph·ªëi m·ªõi c·ªßa \"amount\" (sau khi b·ªè outliers)** \n","metadata":{}},{"cell_type":"code","source":"# ==============================================================\n# X·ª¨ L√ù V√Ä LOG TRANSFORM C·ªòT \"AMOUNT\" - AN TO√ÄN NH·∫§T\n# ==============================================================\n\nprint(\" X·ª≠ l√Ω v√† transform c·ªôt 'amount'...\")\n\n# B∆Ø·ªöC 1: Chuy·ªÉn sang numeric\nprint(\"\\n Chuy·ªÉn ƒë·ªïi dtype...\")\ndf['amount'] = pd.to_numeric(df['amount'], errors='coerce')\nprint(f\"    Dtype: {df['amount'].dtype}\")\n\n# B∆Ø·ªöC 2: X√≥a NaN\nprint(\"\\n X·ª≠ l√Ω NaN...\")\nnan_before = df['amount'].isna().sum()\ndf = df.dropna(subset=['amount'])\nnan_after = df['amount'].isna().sum()\nprint(f\"   ‚Üí ƒê√£ x√≥a {nan_before - nan_after} NaN values\")\n\n# B∆Ø·ªöC 3: X√≥a gi√° tr·ªã <= 0\nprint(\"\\n X·ª≠ l√Ω gi√° tr·ªã kh√¥ng h·ª£p l·ªá...\")\nzero_negative = (df['amount'] <= 0).sum()\nif zero_negative > 0:\n    print(f\"   ‚Üí T√¨m th·∫•y {zero_negative} gi√° tr·ªã <= 0\")\n    df = df[df['amount'] > 0]\n    print(f\"    ƒê√£ x√≥a.  C√≤n l·∫°i: {len(df)} d√≤ng\")\n\n# B∆Ø·ªöC 4: Th·ªëng k√™ tr∆∞·ªõc transform\nprint(\"\\n Th·ªëng k√™ TR∆Ø·ªöC transform:\")\nprint(df['amount'].describe())\n\n# B∆Ø·ªöC 5: Log transformation\nprint(\"\\n √Åp d·ª•ng Log Transformation...\")\ndf['amount_log'] = np.log1p(df['amount'])\nprint(\"    ƒê√£ t·∫°o 'amount_log'\")\n\n# B∆Ø·ªöC 6: Th·ªëng k√™ sau transform\nprint(\"\\n Th·ªëng k√™ SAU transform:\")\nprint(df['amount_log'].describe())\n\n# B∆Ø·ªöC 7: Reset index\ndf = df.reset_index(drop=True)\n\nprint(\"\\n HO√ÄN TH√ÄNH!\")\nprint(f\"üìã Dataset cu·ªëi: {len(df)} d√≤ng, {len(df.columns)} c·ªôt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================\n# V·∫º BI·ªÇU ƒê·ªí SO S√ÅNH: Original vs Log Transformed\n# ==============================================================\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Bi·ªÉu ƒë·ªì 1: D·ªØ li·ªáu g·ªëc (c√≥ outliers, skewed)\ndf[\"amount\"].dropna().plot(kind=\"hist\", bins=30, ax=ax1, color=\"lightcoral\")\nax1.set_title(\"Ph√¢n ph·ªëi Amount G·ªëc\\n(C√≥ outliers, l·ªách ph·∫£i)\", \n              fontsize=12, fontweight=600)\nax1.set_xlabel(\"S·ªë ti·ªÅn (Rs.)\")\nax1.set_ylabel(\"T·∫ßn su·∫•t\")\n\n#  Bi·ªÉu ƒë·ªì 2: Sau Log transformation (normalized)\ndf[\"amount_log\"].dropna().plot(kind=\"hist\", bins=30, ax=ax2, color=\"lightgreen\")\nax2.set_title(\"Ph√¢n ph·ªëi Amount sau Log Transform\\n(ƒê√£ chu·∫©n h√≥a, ph√π h·ª£p ML)\", \n              fontsize=12, fontweight=600)\nax2.set_xlabel(\"Log(Amount + 1)\")\nax2.set_ylabel(\"T·∫ßn su·∫•t\")\n\nplt.tight_layout()\nplt.show()\n\n# So s√°nh ƒë·ªô l·ªách (skewness)\nprint(\"\\n So s√°nh ƒë·ªô l·ªách (Skewness):\")\nprint(f\"   D·ªØ li·ªáu g·ªëc: {df['amount'].skew():.2f} (r·∫•t l·ªách)\")\nprint(f\"   Sau Log transform: {df['amount_log'].skew():.2f} (√≠t l·ªách h∆°n)\")\n\n# Reset index (v√¨ ƒë√£ x√≥a m·ªôt s·ªë d√≤ng)\ndf = df.reset_index(drop=True)\n\nprint(\"\\n Ho√†n th√†nh x·ª≠ l√Ω c·ªôt 'amount'!\")\nprint(f\"Dataset hi·ªán c√≥: {len(df)} d√≤ng, {len(df.columns)} c·ªôt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **4.b.viii.  Ph√¢n t√≠ch c·ªôt \"fulfilment\" (Ngu·ªìn th·ª±c hi·ªán ƒë∆°n h√†ng)**\n\n**M·ª•c ƒë√≠ch:** Ki·ªÉm tra ph√¢n ph·ªëi c·ªßa 2 lo·∫°i fulfilment: Amazon (FBA) vs Merchant (FBM)\n\n**Ph∆∞∆°ng ph√°p:** Value counts + proportion analysis","metadata":{}},{"cell_type":"code","source":"# t·∫°o dataframe ƒë·ªÉ ki·ªÉm tra t·ª∑ l·ªá % c·ªßa 2 lo·∫°i \"fulfilment\" (Merchant vs Amazon)\ndf_cat = df[\"fulfilment\"].value_counts().to_frame()\n# Th√™m c·ªôt t·ª∑ l·ªá %\ndf_cat[\"proportion\"] = round(df[\"fulfilment\"].value_counts(normalize = True)*100,2)\n# Th√™m d·∫•u \"%\" v√†o\ndf_cat[\"proportion\"] = df_cat[\"proportion\"].astype(str).str.cat(np.full((2,),\"%\"))\n# ƒê·ªïi t√™n c√°c c·ªôt cho ƒë·∫πp\ndf_cat = (df_cat.reset_index()\n          .rename(columns = {\"fulfilment\":\"count\",\"index\":\"fulfilment_type\"}))\n# In b·∫£ng ra\ndf_cat","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### NH·∫¨N X√âT:\n\n**Quan s√°t:** Ph√¢n ph·ªëi l·ªách (Merchant ~86%, Amazon ~13%) nh∆∞ng c·∫£ 2 nh√≥m ƒë·ªÅu c√≥ s·ªë l∆∞·ª£ng ƒë·ªß l·ªõn (kh√¥ng qu√° nh·ªè) n√™n v·∫´n t·ªët cho model h·ªçc.\n\n**Quy·∫øt ƒë·ªãnh:** Gi·ªØ nguy√™n, kh√¥ng x·ª≠ l√Ω g√¨ th√™m. ","metadata":{}},{"cell_type":"markdown","source":"### **4.b.ix. Ph√¢n t√≠ch c·ªôt \"saleschannel\" (K√™nh b√°n h√†ng)**\n\n**M·ª•c ƒë√≠ch:** Ki·ªÉm tra c√≥ bao nhi√™u sales channels","metadata":{}},{"cell_type":"code","source":"# ==============================================================\n# PH√ÇN T√çCH C·ªòT \"SALESCHANNEL\"\n# ==============================================================\n\n# T·∫°o dataframe ƒë·ªÉ ki·ªÉm tra t·ª∑ l·ªá %\ndf_cat = df[\"saleschannel\"].value_counts().to_frame()\n\n# Th√™m c·ªôt t·ª∑ l·ªá %\ndf_cat[\"proportion\"] = round(df[\"saleschannel\"].value_counts(normalize=True) * 100, 4)\n\nnum_values = len(df_cat)  # S·ªë l∆∞·ª£ng unique values th·ª±c t·∫ø\n\n# Th√™m d·∫•u \"%\" v√†o (d√πng s·ªë l∆∞·ª£ng th·ª±c t·∫ø)\ndf_cat[\"proportion\"] = df_cat[\"proportion\"].astype(str).str.cat(np.full((num_values,), \"%\"))\n\n# ƒê·ªïi t√™n c√°c c·ªôt cho ƒë·∫πp\ndf_cat = df_cat.reset_index().rename(columns={\"saleschannel\": \"count\", \"index\": \"saleschannel_type\"})\n\n# In b·∫£ng ra\nprint(\" Ph√¢n ph·ªëi Saleschannel:\")\nprint(df_cat)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### ** NH·∫¨N X√âT:**\n\n**Quan s√°t:** \n\nSau qu√° tr√¨nh l√†m s·∫°ch d·ªØ li·ªáu (x√≥a amount=0, qty=0, status kh√¥ng r√µ r√†ng), c·ªôt \"saleschannel\" ch·ªâ c√≤n l·∫°i **100% l√† \"Amazon.  in\"** (35,265 d√≤ng).    D√≤ng duy nh·∫•t \"Non-Amazon\" trong raw data ƒë√£ b·ªã lo·∫°i b·ªè trong c√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω tr∆∞·ªõc ƒë√≥.\n\n**Ph√¢n t√≠ch:**\n\nC·ªôt n√†y kh√¥ng c√≤n cung c·∫•p th√¥ng tin h·ªØu √≠ch (zero variance - kh√¥ng c√≥ s·ª± kh√°c bi·ªát gi·ªØa c√°c d√≤ng).   Trong machine learning, features v·ªõi zero variance kh√¥ng gi√∫p model ph√¢n bi·ªát ƒë∆∞·ª£c gi·ªØa c√°c classes v√¨ t·∫•t c·∫£ samples ƒë·ªÅu c√≥ c√πng m·ªôt gi√° tr·ªã.\n-->X√≥a c·ªôt","metadata":{}},{"cell_type":"code","source":"# X√≥a (drop) c·ªôt \"saleschannel\"\ndf.drop(\"saleschannel\", axis = \"columns\", inplace = True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **4.b.x. Ph√¢n t√≠ch c·ªôt \"shipservicelevel\" (M·ª©c ƒë·ªô d·ªãch v·ª• v·∫≠n chuy·ªÉn)**\n\n**M·ª•c ƒë√≠ch:** Ki·ªÉm tra ph√¢n ph·ªëi Standard vs Expedited shipping","metadata":{}},{"cell_type":"code","source":"# t·∫°o dataframe ƒë·ªÉ ki·ªÉm tra t·ª∑ l·ªá % (Standard vs Expedited)\ndf_cat = df[\"shipservicelevel\"].value_counts().to_frame()\n# Th√™m c·ªôt t·ª∑ l·ªá %\ndf_cat[\"proportion\"] = round(df[\"shipservicelevel\"].value_counts(normalize = True)*100,2)\n# Th√™m d·∫•u \"%\" v√†o\ndf_cat[\"proportion\"] = df_cat[\"proportion\"].astype(str).str.cat(np.full((2,),\"%\"))\n# ƒê·ªïi t√™n c√°c c·ªôt cho ƒë·∫πp\ndf_cat = (df_cat.reset_index()\n          .rename(columns = {\"shipservicelevel\":\"count\",\"index\":\"shipservicelevel_type\"}))\n# In b·∫£ng ra\ndf_cat","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### NH·∫¨N X√âT:\n\n**Quan s√°t:** L·ªách (Standard nhi·ªÅu h∆°n) nh∆∞ng c·∫£ 2 nh√≥m ƒë·ªÅu c√≥ s·ªë l∆∞·ª£ng ƒë·ªß l·ªõn. \n\n**Quy·∫øt ƒë·ªãnh:** Gi·ªØ nguy√™n","metadata":{}},{"cell_type":"markdown","source":"### **4.b. xi. Ph√¢n t√≠ch c·ªôt \"category\" (Lo·∫°i s·∫£n ph·∫©m)**\n\n**M·ª•c ƒë√≠ch:** Ki·ªÉm tra ph√¢n ph·ªëi c√°c lo·∫°i s·∫£n ph·∫©m (kurta, set, saree, etc.)\n\n**Ph∆∞∆°ng ph√°p:** Bar chart visualization","metadata":{}},{"cell_type":"code","source":"# ----- B·∫ÆT ƒê·∫¶U V·∫º BI·ªÇU ƒê·ªí -----\nfig,ax = plt.subplots(figsize = (7,5))\n\n# t·∫°o dataframe ƒë·ªÉ ki·ªÉm tra t·ª∑ l·ªá % c·ªßa c√°c lo·∫°i 'category'\ndf_cat = df[\"category\"].value_counts().to_frame()\n# Th√™m c·ªôt t·ª∑ l·ªá %\ndf_cat[\"proportion\"] = round(df[\"category\"].value_counts(normalize = True)*100,2)\n# Th√™m c·ªôt nh√£n % (ƒë·ªÉ l√°t v·∫Ω l√™n bi·ªÉu ƒë·ªì)\ndf_cat[\"proportion_label\"] = df_cat[\"proportion\"].astype(str).str.cat(np.full((len(df_cat),),\"%\"))\n\n# ƒê·ªïi t√™n c·ªôt 'index' (t√™n c≈©) th√†nh 'category_type'\n# (T·ªõ ƒë√£ s·ª≠a l·∫°i code ·ªü ƒë√¢y cho chu·∫©n)\ndf_cat = df_cat.reset_index().rename(columns = {\"category\":\"category_type\", \"count\":\"count\"})\n\n# V·∫Ω bi·ªÉu ƒë·ªì c·ªôt (bar), X l√† 'category_type', Y l√† 'proportion'\ndf_cat.plot(\n    kind = \"bar\", x = \"category_type\",y = \"proportion\",\n    legend = \"\", ax = ax\n)\n# ƒê·∫∑t Ti√™u ƒë·ªÅ v√† Nh√£n \nplt.xlabel(\"Lo·∫°i H√†ng\")\nplt.ylabel(\"T·ª∑ l·ªá (%)\")\nplt.title('Ph√¢n ph·ªëi c·ªßa c·ªôt \"category\"', fontsize = 14)\nplt.xticks(rotation = 35) # Xoay ch·ªØ tr·ª•c X 35 ƒë·ªô\nplt.yticks(ticks = range(10,51,10) ,labels =[\"10%\",\"20%\",\"30%\",\"40%\",\"50%\"])\n\n# Th√™m nh√£n s·ªë ƒë·∫øm (Count) l√™n tr√™n m·ªói c·ªôt\nfor x,y in enumerate(df_cat[\"count\"]):\n    plt.text(x,1+df_cat[\"proportion\"][x],y,\n             fontsize = 10, horizontalalignment = \"center\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### NH·∫¨N X√âT:\n\n**Quan s√°t:** Ph√¢n ph·ªëi l·ªách (Set, Kurta, Western Dress l√† ch√≠nh) nh∆∞ng t·∫•t c·∫£ categories ƒë·ªÅu c√≥ s·ªë l∆∞·ª£ng ƒë·ªß ƒë·ªÉ model h·ªçc.\n\n**Quy·∫øt ƒë·ªãnh:** Gi·ªØ nguy√™n","metadata":{}},{"cell_type":"markdown","source":"### **4.b.xii. Ph√¢n t√≠ch c·ªôt \"b2b\" (Business-to-Business)**\n\n**M·ª•c ƒë√≠ch:** Ki·ªÉm tra t·ª∑ l·ªá ƒë∆°n h√†ng B2B (b√°n cho doanh nghi·ªáp) vs B2C (b√°n cho c√° nh√¢n)","metadata":{}},{"cell_type":"code","source":"# t·∫°o dataframe ƒë·ªÉ ki·ªÉm tra t·ª∑ l·ªá % (True vs False)\ndf_cat = df[\"b2b\"].value_counts().to_frame()\n# Th√™m c·ªôt t·ª∑ l·ªá %\ndf_cat[\"proportion\"] = round(df[\"b2b\"].value_counts(normalize = True)*100,2)\n# Th√™m d·∫•u \"%\" v√†o\ndf_cat[\"proportion\"] = df_cat[\"proportion\"].astype(str).str.cat(np.full((2,),\"%\"))\n# ƒê·ªïi t√™n c√°c c·ªôt cho ƒë·∫πp\ndf_cat = (df_cat.reset_index()\n          .rename(columns = {\"b2b\":\"count\",\"index\":\"b2b_type\"}))\n# In b·∫£ng ra\ndf_cat","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **NH·∫¨N X√âT:**\n\n**Quan s√°t:** 99.35% l√† B2C (False), ch·ªâ c√≥ 303 ƒë∆°n B2B (True - 0.65%).  Tuy l·ªách nh∆∞ng nh√≥m thi·ªÉu s·ªë v·∫´n c√≥ 303 samples (kh√¥ng qu√° nh·ªè). \n\n**Quy·∫øt ƒë·ªãnh:** \n-  Gi·ªØ l·∫°i feature n√†y\n-  Chuy·ªÉn dtype t·ª´ `bool` ‚Üí `object` ƒë·ªÉ model hi·ªÉu ƒë√¢y l√† categorical (2 lo·∫°i), kh√¥ng ph·∫£i s·ªë 0/1","metadata":{}},{"cell_type":"code","source":"# ƒë·ªïi ki·ªÉu d·ªØ li·ªáu 'b2b' (hi·ªán ƒëang l√† bool True/False) sang 'object' (chu·ªói)\n# Model s·∫Ω hi·ªÉu ƒë√¢y l√† 2 \"lo·∫°i\" (True/False) ch·ª© kh√¥ng ph·∫£i s·ªë (1/0)\ndf[\"b2b\"] = df[\"b2b\"].astype(object)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **4.b.xiii. Ph√¢n t√≠ch c·ªôt \"shipstate\" (Bang giao h√†ng)**\n\n**M·ª•c ƒë√≠ch:** \n- Ki·ªÉm tra ph√¢n ph·ªëi 37 bang c·ªßa ·∫§n ƒê·ªô\n- T·∫°o feature m·ªõi \"region\" (v√πng ƒë·ªãa l√Ω) ƒë·ªÉ gi·∫£m cardinality\n\n**Ph∆∞∆°ng ph√°p: Feature Engineering - Grouping**\n\n·∫§n ƒê·ªô c√≥ 28 states + 8 union territories + 1 capital = 37 administrative divisions, nh∆∞ng c√≥ th·ªÉ gom th√†nh 6 v√πng ƒë·ªãa l√Ω ch√≠nh: North India, South India, East India, West India, Central India, v√† Northeast India.  \n","metadata":{}},{"cell_type":"code","source":"# t·∫°o dataframe ƒë·ªÉ ki·ªÉm tra t·ª∑ l·ªá % c·ªßa c√°c 'shipstate'\ndf_cat = df[\"shipstate\"].value_counts().to_frame()\n# Th√™m c·ªôt t·ª∑ l·ªá %\ndf_cat[\"proportion\"] = round(df[\"shipstate\"].value_counts(normalize = True)*100,2)\n# Th√™m c·ªôt nh√£n % (d·∫°ng chu·ªói)\ndf_cat[\"proportion_str\"] = df_cat[\"proportion\"].astype(str).str.cat(np.full((len(df_cat),),\"%\"))\n\n# ƒê·ªïi t√™n c·ªôt 'index' (t√™n c≈©) th√†nh 'shipstate_type'\n# (T·ªõ ƒë√£ s·ª≠a l·∫°i code ·ªü ƒë√¢y cho chu·∫©n)\ndf_cat = (df_cat.reset_index()\n          .rename(columns = {\"shipstate\":\"shipstate_type\", \"count\":\"count\"}))\n\n# ----- B·∫ÆT ƒê·∫¶U V·∫º BI·ªÇU ƒê·ªí -----\n# S·∫Øp x·∫øp c√°c bang (t·ª´ √≠t -> nhi·ªÅu) ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì ngang (barh) cho ƒë·∫πp\ndf_cat = df_cat.sort_values(by = \"proportion\").reset_index(drop = True)\nfig,ax = plt.subplots(figsize = (7,10)) # Bi·ªÉu ƒë·ªì cao 10 inch\n\n# V·∫Ω bi·ªÉu ƒë·ªì ngang (barh), Y l√† 'shipstate_type', X l√† 'proportion'\ndf_cat.plot(\n    kind = \"barh\", x = \"shipstate_type\",y = \"proportion\",\n    legend = \"\", ax = ax\n)\n# ƒê·∫∑t Ti√™u ƒë·ªÅ v√† Nh√£n (ƒê√É D·ªäCH)\nplt.ylabel(\"Bang giao h√†ng\")\nplt.xlabel(\"T·ª∑ l·ªá (%)\")\nplt.title('Ph√¢n ph·ªëi c·ªßa c·ªôt \"shipstate\"', fontsize = 14)\n# Hi·ªÉn th·ªã t·∫•t c·∫£ 37 t√™n bang tr√™n tr·ª•c Y\nplt.yticks(ticks = list(range(0,len(df_cat))) ,labels=df_cat[\"shipstate_type\"],\n          fontsize = 10)\nplt.xticks(ticks = range(5,26,5) ,labels =[\"5%\",\"10%\",\"15%\",\"20%\",\"25%\"])\n\n# Th√™m nh√£n s·ªë ƒë·∫øm (Count) v√†o cu·ªëi m·ªói thanh ngang\nfor x,y in enumerate(df_cat[\"count\"]):\n    plt.text((df_cat[\"proportion\"][x]),x,y,\n             fontsize = 10, verticalalignment = \"center\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"####  NH·∫¨N X√âT A: Ph√¢n ph·ªëi ban ƒë·∫ßu (37 states)**\n\n**Quan s√°t:** \n\nPh√¢n ph·ªëi r·∫•t l·ªách v·ªõi Maharashtra v√† Karnataka chi·∫øm ph·∫ßn l·ªõn.  M·ªôt s·ªë bang ƒê√¥ng-B·∫Øc (Northeast states) v√† Union Territories c√≥ s·ªë l∆∞·ª£ng r·∫•t nh·ªè (d∆∞·ªõi 100 orders), khi·∫øn model kh√≥ h·ªçc patterns cho nh·ªØng bang n√†y.  Ngo√†i ra, c√≥ 13 d√≤ng \"UNKNOWN\" (kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c bang). \n\n**Quy·∫øt ƒë·ªãnh:**\n1. X√≥a 13 d√≤ng \"UNKNOWN\" (chi·∫øm t·ª∑ l·ªá nh·ªè)\n2. T·∫°o feature m·ªõi \"region\" - gom 37 bang th√†nh 6 v√πng ƒë·ªãa l√Ω\n3. X√≥a c·ªôt \"shipstate\" g·ªëc sau khi c√≥ \"region\"","metadata":{}},{"cell_type":"code","source":"# ==============================================================\n# T·∫†O FEATURE M·ªöI: \"REGION\" (V√πng ƒë·ªãa l√Ω)\n# ==============================================================\n# Feature Engineering: Gom 37 bang ‚Üí 6 v√πng\n# - Gi·∫£m cardinality: 37 ‚Üí 6\n# - Gi·ªØ ƒë∆∞·ª£c th√¥ng tin ƒë·ªãa l√Ω\n# - Model d·ªÖ h·ªçc h∆°n (6 categories thay v√¨ 37)\n\nprint(\"T·∫°o feature: Region\")\n\ndf[\"region\"] = df[\"shipstate\"].replace({\n    # WEST INDIA (T√¢y ·∫§n)\n    \"MAHARASHTRA\": \"westindia\", \"GUJARAT\": \"westindia\",\n    \"RAJASTHAN\": \"westindia\", \"GOA\": \"westindia\",\n    \"DADRA AND NAGAR\": \"westindia\",\n    \n    # SOUTH INDIA (Nam ·∫§n)\n    \"KARNATAKA\": \"southindia\", \"TAMIL NADU\": \"southindia\",\n    \"KERALA\": \"southindia\", \"ANDHRA PRADESH\": \"southindia\",\n    \"TELANGANA\": \"southindia\", \"PUDUCHERRY\": \"southindia\",\n    \"LAKSHADWEEP\": \"southindia\",\n    \n    # NORTH INDIA (B·∫Øc ·∫§n)\n    \"UTTAR PRADESH\": \"northindia\", \"HARYANA\": \"northindia\",\n    \"PUNJAB\": \"northindia\", \"UTTARAKHAND\": \"northindia\",\n    \"HIMACHAL PRADESH\": \"northindia\", \"JAMMU & KASHMIR\": \"northindia\",\n    \"CHANDIGARH\": \"northindia\", \"LADAKH\": \"northindia\",\n    \n    # EAST INDIA (ƒê√¥ng ·∫§n)\n    \"WEST BENGAL\": \"eastindia\", \"JHARKHAND\": \"eastindia\",\n    \"ODISHA\": \"eastindia\", \"BIHAR\": \"eastindia\",\n    \"CHHATTISGARH\": \"eastindia\", \"ANDAMAN & NICOBAR \": \"eastindia\",\n    \n    # CENTRAL INDIA (Trung ·∫§n)\n    \"MADHYA PRADESH\": \"centralindia\", \"NEW DELHI\": \"centralindia\",\n    \n    # NORTHEAST INDIA (ƒê√¥ng B·∫Øc ·∫§n)\n    \"ASSAM\": \"northeastindia\", \"MEGHALAYA\": \"northeastindia\",\n    \"TRIPURA\": \"northeastindia\", \"SIKKIM\": \"northeastindia\",\n    \"MANIPUR\": \"northeastindia\", \"MIZORAM\": \"northeastindia\",\n    \"NAGALAND\": \"northeastindia\", \"ARUNACHAL PRADESH\": \"northeastindia\"\n})\n\n# X√≥a c√°c d√≤ng \"UNKNOWN\"\nbefore_drop = len(df)\ndf = df[df[\"shipstate\"] != \"UNKNOWN\"]\nafter_drop = len(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================\n# V·∫º BI·ªÇU ƒê·ªí: Ph√¢n ph·ªëi \"REGION\" (6 v√πng)\n# ==============================================================\n\ndf_cat = df[\"region\"].value_counts().to_frame()\ndf_cat[\"proportion\"] = round(df[\"region\"].value_counts(normalize=True)*100, 2)\ndf_cat[\"proportion_str\"] = df_cat[\"proportion\"].astype(str).str.cat(np.full((len(df_cat),), \"%\"))\ndf_cat = df_cat.reset_index().rename(columns={\"region\":\"region_name\", \"count\":\"count\"})\ndf_cat = df_cat.sort_values(by=\"proportion\").reset_index(drop=True)\n\nfig, ax = plt.subplots(figsize=(7, 5))\ndf_cat.plot(kind=\"barh\", x=\"region_name\", y=\"proportion\", legend=\"\", ax=ax)\n\nplt.ylabel(\"V√πng\")\nplt.xlabel(\"T·ª∑ l·ªá (%)\")\nplt.title('Ph√¢n ph·ªëi c·ªßa c·ªôt \"region\"', fontsize=14)\nplt.yticks(ticks=list(range(0, len(df_cat))), labels=df_cat[\"region_name\"])\nplt.xticks(ticks=range(10, 51, 10), labels=[\"10%\", \"20%\", \"30%\", \"40%\", \"50%\"])\n\nfor x, y in enumerate(df_cat[\"count\"]):\n    plt.text((df_cat[\"proportion\"][x]), x, y, fontsize=10, verticalalignment=\"center\")\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **NH·∫¨N X√âT B: Sau khi t·∫°o \"region\"**\n\n**Quan s√°t:** \n\nPh√¢n ph·ªëi 6 v√πng v·∫´n l·ªách (South India v√† West India chi·∫øm ph·∫ßn l·ªõn) nh∆∞ng T·∫§T C·∫¢ c√°c v√πng ƒë·ªÅu c√≥ s·ªë l∆∞·ª£ng ƒë·ªß l·ªõn (ngay c·∫£ Northeast India c√≥ s·ªë l∆∞·ª£ng th·∫•p nh·∫•t c≈©ng v·∫´n c√≥ v√†i trƒÉm orders). ƒêi·ªÅu n√†y t·ªët h∆°n nhi·ªÅu so v·ªõi vi·ªác c√≥ 37 categories v·ªõi m·ªôt s·ªë bang ch·ªâ c√≥ v√†i ch·ª•c orders.\n\n**K·∫øt qu·∫£:**\n-  Gi·∫£m cardinality: 37 states ‚Üí 6 regions\n-  T·∫•t c·∫£ regions ƒë·ªÅu c√≥ sample size ƒë·ªß l·ªõn ƒë·ªÉ model h·ªçc\n-  V·∫´n gi·ªØ ƒë∆∞·ª£c th√¥ng tin ƒë·ªãa l√Ω quan tr·ªçng\n\n**Quy·∫øt ƒë·ªãnh cu·ªëi:** \n- Gi·ªØ \"region\" \n- X√≥a \"shipstate\" (redundant)","metadata":{}},{"cell_type":"code","source":"# X√≥a (drop) c·ªôt \"shipstate\" g·ªëc (v√¨ ƒë√£ c√≥ c·ªôt \"region\" thay th·∫ø)\ndf.drop(\"shipstate\",axis = \"columns\", inplace = True)\n\n# reset (ƒë·∫∑t l·∫°i) ch·ªâ m·ª•c (index) c·ªßa dataframe sau khi ƒë√£ x√≥a nhi·ªÅu d√≤ng\ndf = df.reset_index(drop = True)\n# Ki·ªÉm tra l·∫°i th√¥ng tin l·∫ßn cu·ªëi, xem c√≤n bao nhi√™u c·ªôt\ndf.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"final_wrangle\"></a>\n## 4.c. FINAL WRANGLE FUNCTION\n**M·ª•c ƒë√≠ch:** \nT·ªïng h·ª£p T·∫§T C·∫¢ c√°c b∆∞·ªõc l√†m s·∫°ch ƒë√£ ph√¢n t√≠ch ·ªü EDA v√†o m·ªôt h√†m duy nh·∫•t `final_wrangle(filepath)` ƒë·ªÉ c√≥ th·ªÉ t√°i s·ª≠ d·ª•ng. \n\n**Ph∆∞∆°ng ph√°p: ETL Pipeline**\n- **Extract:** ƒê·ªçc file CSV\n- **Transform:** √Åp d·ª•ng t·∫•t c·∫£ cleaning rules\n- **Load:** Return DataFrame s·∫°ch, s·∫µn s√†ng cho modeling","metadata":{}},{"cell_type":"code","source":"# ==============================================================\n# H√ÄM FINAL_WRANGLE (X·ª≠ l√Ω v√† L√†m s·∫°ch D·ªØ li·ªáu T·ªïng h·ª£p)\n# ==============================================================\n\ndef final_wrangle(filepath):\n    # B∆Ø·ªöC 1: ƒê·ªçc file\n    print(\" ƒêang ti·∫øn h√†nh ƒë·ªçc d·ªØ li·ªáu t·ª´ t·∫≠p tin...\")\n    df = pd.read_csv(filepath, low_memory=False)\n    print(f\"   ‚Üí ƒê√£ t·∫£i th√†nh c√¥ng {len(df)} d√≤ng v√† {len(df.columns)} c·ªôt.\")\n    \n    # B∆Ø·ªöC 2: Chu·∫©n h√≥a t√™n c·ªôt (Clean column names)\n    col = [element.lower().replace(\" \", \"\").replace(\"-\", \"\") for element in df.columns]\n    df.columns = col\n    \n    # B∆Ø·ªöC 3: Lo·∫°i b·ªè c√°c thu·ªôc t√≠nh d∆∞ th·ª´a (Drop redundant columns)\n    df.drop([\"index\", \"date\", \"fulfilledby\", \"currency\", \"unnamed:22\",\n             \"promotionids\", \"courierstatus\", \"shipcountry\"],\n            axis=\"columns\", inplace=True)\n    \n    # B∆Ø·ªöC 4: X·ª≠ l√Ω gi√° tr·ªã thi·∫øu cho th√¥ng tin ƒë·ªãa l√Ω (Fill NaN for location)\n    df[\"shipstate\"].fillna(\"unknown\", inplace=True)\n    df[\"shipcity\"].fillna(\"unknown\", inplace=True)\n    df[\"shippostalcode\"].fillna(0, inplace=True)\n    df[\"shippostalcode\"] = df[\"shippostalcode\"].astype(int).astype(object)\n    \n    # B∆Ø·ªöC 5: Chu·∫©n h√≥a t√™n bang (Standardize shipstate)\n    df[\"shipstate\"] = df[\"shipstate\"].str.upper()\n    df[\"shipstate\"].replace({\n        \"PONDICHERRY\": \"PUDUCHERRY\", \"RAJSHTHAN\": \"RAJASTHAN\", \"RAJSTHAN\": \"RAJASTHAN\",\n        \"RJ\": \"RAJASTHAN\", \"PB\": \"PUNJAB\", \"PUNJAB/MOHALI/ZIRAKPUR\": \"PUNJAB\",\n        \"ORISSA\": \"ODISHA\", \"DELHI\": \"NEW DELHI\", \"NL\": \"UNKNOWN\", \"APO\": \"UNKNOWN\", \"AR\": \"UNKNOWN\"\n    }, inplace=True)\n    \n    # B∆Ø·ªöC 6: Lo·∫°i b·ªè c√°c b·∫£n ghi tr√πng l·∫∑p (Drop duplicates)\n    df.drop_duplicates(inplace=True)\n    \n    # B∆Ø·ªöC 7-9: Thu h·∫πp ph·∫°m vi nghi√™n c·ª©u: Ch·ªâ gi·ªØ ƒë∆°n h√†ng ƒë∆°n l·∫ª (Remove multi-product orders)\n    df = df[df[\"orderid\"].duplicated(keep=False) == False]\n    df.drop(\"orderid\", axis=1, inplace=True)\n    \n    # B∆Ø·ªöC 10: Thi·∫øt l·∫≠p bi·∫øn m·ª•c ti√™u \"rejected\" (Create target variable)\n    known_value = [\"Cancelled\", 'Shipped - Returned to Seller', 'Shipped - Rejected by Buyer',\n                   'Shipped - Returning to Seller', 'Shipped - Delivered to Buyer']\n    df = df[df[\"status\"].isin(known_value)]\n    \n    rejected = [\"Cancelled\", 'Shipped - Returned to Seller', 'Shipped - Rejected by Buyer',\n                'Shipped - Returning to Seller']\n    df[\"rejected\"] = df[\"status\"].isin(rejected).astype(int)\n    df.drop(\"status\", axis=\"columns\", inplace=True)\n    \n    # B∆Ø·ªöC 11: Lo·∫°i b·ªè ƒë·∫∑c tr∆∞ng c√≥ s·ªë chi·ªÅu cao (Drop high cardinality features)\n    df.drop([\"style\", \"sku\", \"shipcity\", \"shippostalcode\", \"asin\"], axis=1, inplace=True)\n    \n    # B∆Ø·ªöC 12: X·ª¨ L√ù S·ªê L∆Ø·ª¢NG (QTY) ‚Üí Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng nh·ªã ph√¢n\n    print(\"\\n ƒêang x·ª≠ l√Ω thu·ªôc t√≠nh 'qty' (S·ªë l∆∞·ª£ng)...\")\n    \n    df.drop(\"qty\", axis=\"columns\", inplace=True)\n    print(\"   ‚Üí ƒê√£ x√≥a c·ªôt 'qty'\")\n    \n    # B∆Ø·ªöC 13: X·ª¨ L√ù GI√Å TR·ªä ƒê∆†N H√ÄNG (AMOUNT) ‚Üí Bi·∫øn ƒë·ªïi Logarit\n    print(\"\\n ƒêang x·ª≠ l√Ω thu·ªôc t√≠nh 'amount' (Gi√° tr·ªã ƒë∆°n h√†ng)...\")\n    \n    # 13.1.  CHUY·ªÇN ƒê·ªîI DTYPE SANG NUMERIC (QUAN TR·ªåNG!)\n    print(\"   ‚Üí Chuy·ªÉn ƒë·ªïi dtype sang numeric...\")\n    df[\"amount\"] = pd.to_numeric(df[\"amount\"], errors='coerce')\n    print(f\"   ‚Üí Dtype hi·ªán t·∫°i: {df['amount'].dtype}\")\n    \n    # 13.2.  Ki·ªÉm tra v√† x·ª≠ l√Ω NaN\n    nan_count = df[\"amount\"].isna().sum()\n    if nan_count > 0:\n        print(f\"   ‚Üí T√¨m th·∫•y {nan_count} gi√° tr·ªã NaN, ƒëang x√≥a...\")\n        df = df.dropna(subset=[\"amount\"])\n        print(f\"   ‚Üí ƒê√£ x√≥a {nan_count} d√≤ng c√≥ NaN\")\n    \n    # 13.3. X√≥a c√°c d√≤ng c√≥ amount = 0\n    amount_zero_count = (df[\"amount\"] == 0).sum()\n    if amount_zero_count > 0:\n        print(f\"   ‚Üí T√¨m th·∫•y {amount_zero_count} d√≤ng c√≥ amount=0\")\n        df = df[df[\"amount\"] != 0]\n        print(f\"   ‚Üí ƒê√£ x√≥a {amount_zero_count} d√≤ng\")\n    \n    # 13. 4. X√≥a c√°c gi√° tr·ªã √¢m (n·∫øu c√≥)\n    negative_count = (df[\"amount\"] < 0).sum()\n    if negative_count > 0:\n        print(f\"   ‚Üí T√¨m th·∫•y {negative_count} gi√° tr·ªã √¢m, ƒëang x√≥a...\")\n        df = df[df[\"amount\"] > 0]\n    \n    # 13.5.  √ÅP D·ª§NG LOG TRANSFORMATION (B√¢y gi·ªù s·∫Ω ho·∫°t ƒë·ªông!)\n    print(\"   ‚Üí √Åp d·ª•ng Log Transformation...\")\n    df[\"amount_log\"] = np.log1p(df[\"amount\"])\n    print(\"   ƒê√£ t·∫°o feature 'amount_log'\")\n    \n    print(f\"   ‚Üí S·ªë d√≤ng c√≤n l·∫°i: {len(df)}\")\n    \n    # B∆Ø·ªöC 14: Lo·∫°i b·ªè \"saleschannel\"\n    df.drop(\"saleschannel\", axis=\"columns\", inplace=True)\n    \n    # B∆Ø·ªöC 15: T·∫°o ƒë·∫∑c tr∆∞ng v√πng mi·ªÅn \"region\" (Feature Engineering)\n    print(\"\\n ƒêang th·ª±c hi·ªán tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng 'region' (V√πng mi·ªÅn)...\")\n    df[\"region\"] = df[\"shipstate\"].replace({\n        \"MAHARASHTRA\": \"westindia\", \"KARNATAKA\": \"southindia\",\n        \"PUDUCHERRY\": \"southindia\", \"TELANGANA\": \"southindia\",\n        \"ANDHRA PRADESH\": \"southindia\", \"HARYANA\": \"northindia\",\n        \"JHARKHAND\": \"eastindia\", \"CHHATTISGARH\": \"eastindia\",\n        \"ASSAM\": \"northeastindia\", \"ODISHA\": \"eastindia\",\n        \"UTTAR PRADESH\": \"northindia\", \"GUJARAT\": \"westindia\",\n        \"TAMIL NADU\": \"southindia\", \"UTTARAKHAND\": \"northindia\",\n        \"WEST BENGAL\": \"eastindia\", \"RAJASTHAN\": \"westindia\",\n        \"NEW DELHI\": \"centralindia\", \"MADHYA PRADESH\": \"centralindia\",\n        \"KERALA\": \"southindia\", \"JAMMU & KASHMIR\": \"northindia\",\n        \"BIHAR\": \"eastindia\", \"MEGHALAYA\": \"northeastindia\",\n        \"PUNJAB\": \"northindia\", \"GOA\": \"southindia\",\n        \"TRIPURA\": \"northeastindia\", \"CHANDIGARH\": \"northindia\",\n        \"HIMACHAL PRADESH\": \"northindia\", \"SIKKIM\": \"northeastindia\",\n        \"ANDAMAN & NICOBAR \": \"eastindia\", \"MANIPUR\": \"northeastindia\",\n        \"MIZORAM\": \"northeastindia\", \"NAGALAND\": \"northeastindia\",\n        \"ARUNACHAL PRADESH\": \"northeastindia\", \"LADAKH\": \"northindia\",\n        \"DADRA AND NAGAR\": \"westindia\", \"LAKSHADWEEP\": \"southindia\"\n    })\n    \n    # Lo·∫°i b·ªè c√°c bang kh√¥ng x√°c ƒë·ªãnh\n    df = df[df[\"shipstate\"] != \"UNKNOWN\"]\n    df.drop(\"shipstate\", axis=\"columns\", inplace=True)\n    print(\"   ‚Üí ƒê√£ t·∫°o ƒë·∫∑c tr∆∞ng 'region' (g·ªìm 6 v√πng ƒë·ªãa l√Ω).\")\n    \n    # B∆Ø·ªöC 16: Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu b2b\n    df[\"b2b\"] = df[\"b2b\"].astype(object)\n    \n    # B∆Ø·ªöC 17: ƒê·∫∑t l·∫°i ch·ªâ m·ª•c\n    df = df.reset_index(drop=True)\n    \n    # B∆Ø·ªöC 18: KI·ªÇM TRA CH·∫§T L∆Ø·ª¢NG D·ªÆ LI·ªÜU CU·ªêI C√ôNG (FINAL DATA QUALITY CHECK)\n    print(\"\\n Ki·ªÉm tra ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu l·∫ßn cu·ªëi...\")\n    nan_count = df.isna().sum()\n    total_nans = nan_count.sum()\n    \n    if total_nans > 0:\n        print(\"   C·∫¢NH B√ÅO: Ph√°t hi·ªán gi√° tr·ªã thi·∫øu (NaN) trong d·ªØ li·ªáu:\")\n        print(nan_count[nan_count > 0])\n        raise ValueError(\"   L·ªñI: DataFrame ch·ª©a gi√° tr·ªã NaN! Qu√° tr√¨nh SMOTE s·∫Ω th·∫•t b·∫°i!\")\n    else:\n        print(\"   Kh√¥ng ph√°t hi·ªán gi√° tr·ªã thi·∫øu. D·ªØ li·ªáu s·∫µn s√†ng cho SMOTE!\")\n    \n    print(f\"\\n B·ªô d·ªØ li·ªáu ho√†n ch·ªânh: {len(df)} d√≤ng, {len(df.columns)} c·ªôt.\")\n    print(f\"   Danh s√°ch c·ªôt: {df.columns.tolist()}\")\n    \n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **5. DATA LOADING (T·∫£i d·ªØ li·ªáu ƒë√£ l√†m s·∫°ch)**\n\n**M·ª•c ƒë√≠ch:** √Åp d·ª•ng h√†m `final_wrangle()` ƒë·ªÉ c√≥ ƒë∆∞·ª£c dataset cu·ªëi c√πng, s·∫°ch, s·∫µn s√†ng cho modeling\n\n**Ph∆∞∆°ng ph√°p: ETL Pipeline Execution**\n- Execute to√†n b·ªô cleaning pipeline m·ªôt l·∫ßn duy nh·∫•t\n- Verify k·∫øt qu·∫£ (shape, columns, no NaN)","metadata":{}},{"cell_type":"code","source":"# 5. T·∫¢I D·ªÆ LI·ªÜU - TH·ª∞C THI H√ÄM FINAL_WRANGLE()\nfilepath_list = glob(\"/kaggle/input/unlock-profits-with-e-commerce-sales-data/*.csv\")\nfilepath = filepath_list[0]\n\nprint(\"ƒêANG TH·ª∞C THI FINAL_WRANGLE() - QUY TR√åNH ETL TO√ÄN DI·ªÜN\")\n\n# Ch·∫°y h√†m l√†m s·∫°ch v√† x·ª≠ l√Ω d·ªØ li·ªáu\ndf = final_wrangle(filepath)\n\n\nprint(\" QU√Å TR√åNH T·∫¢I D·ªÆ LI·ªÜU HO√ÄN T·∫§T!\")\n\n#  Ki·ªÉm tra k·∫øt qu·∫£ ƒë·∫ßu ra\nprint(\"\\n Th√¥ng tin B·ªô d·ªØ li·ªáu Cu·ªëi c√πng (Final Dataset Info):\")\ndf.info()\n\nprint(\"\\n 5 d√≤ng d·ªØ li·ªáu ƒë·∫ßu ti√™n:\")\ndisplay(df.head())\n\n# KI·ªÇM TRA T√çNH H·ª¢P L·ªÜ (VALIDATION CHECK)\nprint(\"\\n C√°c ki·ªÉm tra t√≠nh h·ª£p l·ªá:\")\nprint(f\"    K√≠ch th∆∞·ªõc (Shape): {df.shape}\")\nprint(f\"    Kh√¥ng c√≥ d·ªØ li·ªáu tr√πng l·∫∑p: {df.duplicated().sum() == 0}\")\nprint(f\"    Kh√¥ng c√≥ gi√° tr·ªã thi·∫øu (NaN): {df.isna().sum().sum() == 0}\")\nprint(f\"    Bi·∫øn m·ª•c ti√™u 'rejected' t·ªìn t·∫°i: {'rejected' in df.columns}\")\nprint(f\"    Ph√¢n ph·ªëi bi·∫øn m·ª•c ti√™u: {df['rejected'].value_counts().to_dict()}\")\n\nprint(\"\\n B·ªô d·ªØ li·ªáu ƒë√£ s·∫µn s√†ng cho giai ƒëo·∫°n ti·ªÅn x·ª≠ l√Ω v√† m√¥ h√¨nh h√≥a!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id = \"data_preprocessing\"></a>\n# **6. DATA PREPROCESSING**","metadata":{}},{"cell_type":"markdown","source":"<a id = \"split_dataset\"></a>\n## <br>**6.a. Chia Dataset**<br>\n\n### **6.a.i.  Chia Dataset th√†nh X (features) v√† y (target)**\n\n**M·ª•c ƒë√≠ch:** T√°ch bi·∫øn m·ª•c ti√™u (\"rejected\") ra kh·ªèi c√°c features\n\n**Ph∆∞∆°ng ph√°p: Train-Test Split Preparation**","metadata":{"execution":{"iopub.status.busy":"2023-01-22T10:44:42.876337Z","iopub.execute_input":"2023-01-22T10:44:42.876815Z","iopub.status.idle":"2023-01-22T10:44:42.884748Z","shell.execute_reply.started":"2023-01-22T10:44:42.876776Z","shell.execute_reply":"2023-01-22T10:44:42.883708Z"}}},{"cell_type":"code","source":"# C·ªôt \"m·ª•c ti√™u\" (c√°i c·∫ßn d·ª± ƒëo√°n)\ntarget = \"rejected\"\ny = df[target]     # y = vector m·ª•c ti√™u (ch·ªâ ch·ª©a c·ªôt \"rejected\")\nX = df.drop(target, axis = \"columns\")   # X = ma tr·∫≠n ƒë·∫∑c tr∆∞ng \n\n# Hi·ªÉn th·ªã th√¥ng tin ƒë·ªÉ ki·ªÉm tra\n\nprint(\"Split into X and y:\")\nprint(f\"   ‚Üí y shape: {y.shape}\")\nprint(f\"   ‚Üí X shape: {X.shape}\")\nprint(f\"\\n Features (X columns): {X.columns.tolist()}\")\nprint(f\"\\n Target distribution (y):\")\nprint(y.value_counts())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **6.a.ii. Chia th√†nh Train v√† Test sets**\n\n**M·ª•c ƒë√≠ch:** Chia data th√†nh 80% train, 20% test\n\n**Ph∆∞∆°ng ph√°p: Stratified Split**\n\nS·ª≠ d·ª•ng stratified split cho imbalanced data ƒë·ªÉ ƒë·∫£m b·∫£o t·ª∑ l·ªá target classes gi·ªëng nhau ·ªü c·∫£ train v√† test sets.  Parameter `stratify=y` trong `train_test_split()` s·∫Ω t·ª± ƒë·ªông maintain t·ª∑ l·ªá 58:42 (Not Rejected:Rejected) ·ªü c·∫£ 2 sets, tr√°nh tr∆∞·ªùng h·ª£p test set c√≥ distribution kh√°c train set (data distribution shift).","metadata":{}},{"cell_type":"code","source":"# Ph√¢n chia d·ªØ li·ªáu c√≥ ph√¢n t·∫ßng (stratification)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=0.2, \n    random_state=42,\n    stratify=y  # Th√™m d√≤ng n√†y ƒë·ªÉ c√¢n b·∫±ng t·ª∑ l·ªá l·ªõp d·ªØ li·ªáu\n)\n\nprint(\" Ho√†n t·∫•t ph√¢n chia t·∫≠p Hu·∫•n luy·ªán - Ki·ªÉm th·ª≠:\")\nprint(f\"   ‚Üí X_train (ƒê·∫∑c tr∆∞ng hu·∫•n luy·ªán): {X_train.shape}\")\nprint(f\"   ‚Üí X_test (ƒê·∫∑c tr∆∞ng ki·ªÉm th·ª≠): {X_test.shape}\")\nprint(f\"   ‚Üí y_train (Nh√£n hu·∫•n luy·ªán): {y_train.shape}\")\nprint(f\"   ‚Üí y_test (Nh√£n ki·ªÉm th·ª≠): {y_test.shape}\")\n\n# Ki·ªÉm tra ph√¢n ph·ªëi l·ªõp d·ªØ li·ªáu\nprint(\"\\n Ph√¢n ph·ªëi l·ªõp trong t·∫≠p HU·∫§N LUY·ªÜN:\")\nprint(y_train.value_counts(normalize=True))\n\nprint(\"\\n Ph√¢n ph·ªëi l·ªõp trong t·∫≠p KI·ªÇM TH·ª¨:\")\nprint(y_test.value_counts(normalize=True))\n\nprint(\"\\n C√°c ph√¢n ph·ªëi t∆∞∆°ng ƒë·ªìng (K·ªπ thu·∫≠t ph√¢n t·∫ßng d·ªØ li·ªáu ho·∫°t ƒë·ªông hi·ªáu qu·∫£!)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **6. b. X·ª≠ l√Ω Imbalanced Data**\n\n**M·ª•c ƒë√≠ch:** C√¢n b·∫±ng train set b·∫±ng SMOTE (ch·ªâ oversample train, kh√¥ng ƒë·ªông v√†o test)\n\n**Ph∆∞∆°ng ph√°p: SMOTENC (SMOTE for mixed data)**\n\nSMOTENC l√† phi√™n b·∫£n m·ªü r·ªông cho ph√©p x·ª≠ l√Ω d·ªØ li·ªáu h·ªón h·ª£p (categorical + numerical).  \nQuan tr·ªçng: ch·ªâ √°p d·ª•ng SMOTE tr√™n train set ƒë·ªÉ tr√°nh data leakage (n·∫øu oversample c·∫£ test set th√¨ model s·∫Ω \"nh√¨n th·∫•y\" test data trong qu√° tr√¨nh train).","metadata":{}},{"cell_type":"markdown","source":"### **6.b.i.  Ki·ªÉm tra s·ª± m·∫•t c√¢n b·∫±ng trong y_train**\n\n**M·ª•c ƒë√≠ch:** Visualize ph√¢n ph·ªëi target trong train set tr∆∞·ªõc khi oversample","metadata":{}},{"cell_type":"code","source":"# ==============================================================\n# 6.b.i. KI·ªÇM TRA IMBALANCE TRONG TRAIN SET\n# ==============================================================\n\n# V·∫Ω bi·ªÉu ƒë·ªì ph√¢n ph·ªëi target trong train set (TR∆Ø·ªöC SMOTE)\nfig, ax = plt.subplots(figsize=(7, 5))\n\n# T√≠nh t·ª∑ l·ªá % c·ªßa 2 classes (0 v√† 1)\nplot_dataseries = round(y_train.value_counts(normalize=True) * 100, 2)\n\n# V·∫Ω bi·ªÉu ƒë·ªì c·ªôt\nplot_dataseries.plot(kind=\"bar\", ax=ax, color=\"navy\")\n\n# Th√™m text c·∫£nh b√°o\nplt.text(0.5, 70, \"Dataset L·ªõp M·∫•t c√¢n b·∫±ng\", color=\"darkred\",\n         horizontalalignment=\"center\", fontsize=14, fontweight=700)\nplt.axhline(y=plot_dataseries[0], color=\"darkred\", linestyle=\"--\")\n\n# Ti√™u ƒë·ªÅ v√† nh√£n\nplt.title(\"T·ª∑ l·ªá c√°c L·ªõp c·ªßa y_train (Tr∆∞·ªõc SMOTE)\", fontsize=16)\nplt.ylabel(\"T·ª∑ l·ªá (%)\", fontsize=12)\nplt.xlabel(\"Tr·∫°ng th√°i\", fontsize=12)\nplt.xticks(ticks=range(len(plot_dataseries)),\n           labels=[\"Kh√¥ng b·ªã t·ª´ ch·ªëi (0)\", \"B·ªã t·ª´ ch·ªëi (1)\"], \n           rotation=\"horizontal\")\nplt.yticks(ticks=[20, 40, 60, 80, 100], labels=[\"20%\", \"40%\", \"60%\", \"80%\", \"100%\"])\n\n# Th√™m nh√£n % l√™n c·ªôt\ndata_label = plot_dataseries.astype(str).str.cat(np.full((2,), \"%\"), sep=\"\")\nfor x, y in enumerate(plot_dataseries):\n    plt.text(x, y-7, data_label[x], color=\"white\",\n             fontweight=700, fontsize=13, horizontalalignment=\"center\")\n\n# Th√™m nh√£n s·ªë ƒë·∫øm (Count)\nfor x, y in enumerate(y_train.value_counts()):\n    plt.text(x, plot_dataseries[x]-13, \n             f\"S·ªë l∆∞·ª£ng: {y_train.value_counts()[x]}\",\n             horizontalalignment=\"center\", color=\"lightpink\", \n             fontsize=12, fontweight=700)\n\nplt.show()\n\n# In k·∫øt qu·∫£ ra console\nprint(\"Ph√¢n ph·ªëi y_train (TR∆Ø·ªöC SMOTE):\")\nprint(y_train.value_counts())\nprint(\"\\nT·ª∑ l·ªá %:\")\nprint(y_train.value_counts(normalize=True) * 100)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **6.b.ii.  √Åp d·ª•ng SMOTE ƒë·ªÉ c√¢n b·∫±ng train set**\n\n**M·ª•c ƒë√≠ch:** S·ª≠ d·ª•ng SMOTENC ƒë·ªÉ t·∫°o synthetic samples cho minority class, ƒë·∫°t t·ª∑ l·ªá 50-50\n\n**SMOTE ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o:**\n\nSMOTE t·∫°o synthetic samples m·ªõi b·∫±ng c√°ch n·ªôi suy (interpolate) gi·ªØa c√°c minority samples hi·ªán c√≥.  C·ª• th·ªÉ: \n(1) Ch·ªçn m·ªôt minority sample, (2) T√¨m k nearest neighbors c·ªßa n√≥, (3) Ch·ªçn ng·∫´u nhi√™n m·ªôt neighbor, (4) T·∫°o synthetic sample n·∫±m tr√™n ƒëo·∫°n th·∫≥ng n·ªëi gi·ªØa sample g·ªëc v√† neighbor ƒë√≥.  ƒêi·ªÅu n√†y gi√∫p model h·ªçc ƒë∆∞·ª£c v√πng decision boundary t·ªët h∆°n thay v√¨ ch·ªâ \"ghi nh·ªõ\" c√°c samples b·ªã duplicate. ","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTENC\n\nprint(\" √ÅP D·ª§NG SMOTENC (SMOTE for Mixed Categorical + Numerical Data)\")\nprint()\n\n# B∆Ø·ªöC 1: X√ÅC ƒê·ªäNH CATEGORICAL FEATURES\n# SMOTENC c·∫ßn bi·∫øt c·ªôt n√†o l√† categorical ƒë·ªÉ x·ª≠ l√Ω ƒë√∫ng\n# ‚Üí L·∫•y index (v·ªã tr√≠) c·ªßa c√°c c·ªôt categorical (dtype='object')\n\ncategorical_features = []\nfor i, col in enumerate(X_train.columns):\n    if X_train[col].dtype == 'object':\n        categorical_features.append(i)\n\nprint(\"Th√¥ng tin v·ªÅ features:\")\nprint(f\"   ‚Üí T·ªïng s·ªë features: {len(X_train.columns)}\")\nprint(f\"   ‚Üí Categorical feature indices: {categorical_features}\")\nprint(f\"   ‚Üí Categorical features: {[X_train.columns[i] for i in categorical_features]}\")\nprint(f\"   ‚Üí Numerical features: {list(X_train.select_dtypes(exclude='object').columns)}\")\nprint()\n\n# B∆Ø·ªöC 2: KH·ªûI T·∫†O SMOTENC\n#  T·∫°o SMOTENC instance\nsmote = SMOTENC(\n    categorical_features=categorical_features,  # Ch·ªâ ƒë·ªãnh c·ªôt n√†o l√† categorical\n    random_state=42,  # ƒê·ªÉ reproducible (ch·∫°y l·∫°i ra k·∫øt qu·∫£ gi·ªëng nhau)\n    k_neighbors=5  # S·ªë neighbors d√πng ƒë·ªÉ t·∫°o synthetic samples\n)\nprint(\" SMOTENC Parameters:\")\nprint(f\"   ‚Üí k_neighbors: 5 (m·ªói synthetic sample t·∫°o t·ª´ 5 neighbors g·∫ßn nh·∫•t)\")\nprint(f\"   ‚Üí random_state: 42 (ƒë·∫£m b·∫£o reproducibility)\")\nprint()\n# B∆Ø·ªöC 3: FIT-RESAMPLE (Ch·ªâ tr√™n TRAIN SET!)\n\nprint(\" ƒêang √°p d·ª•ng SMOTE tr√™n train set...\")\nprint(f\"   ‚Üí Train set TR∆Ø·ªöC SMOTE: {X_train.shape}\")\nprint(f\"   ‚Üí y_train TR∆Ø·ªöC SMOTE: {y_train.value_counts().to_dict()}\")\nprint()\n\n#  Fit v√† resample\nX_train_over, y_train_over = smote.fit_resample(X_train, y_train)\n\nprint(\" SMOTE ho√†n th√†nh!\")\nprint(f\"   ‚Üí Train set SAU SMOTE: {X_train_over.shape}\")\nprint(f\"   ‚Üí y_train SAU SMOTE: {y_train_over.value_counts().to_dict()}\")\nprint()\n\n# B∆Ø·ªöC 4: KI·ªÇM TRA K·∫æT QU·∫¢\n\nprint(\" Ph√¢n ph·ªëi class sau SMOTE:\")\nprint(y_train_over.value_counts())\nprint(\"\\nT·ª∑ l·ªá %:\")\nprint(y_train_over.value_counts(normalize=True) * 100)\nprint()\n\n# Ki·ªÉm tra c√¢n b·∫±ng\nclass_0_pct = y_train_over.value_counts(normalize=True)[0] * 100\nclass_1_pct = y_train_over.value_counts(normalize=True)[1] * 100\n\nif abs(class_0_pct - 50) < 0.1 and abs(class_1_pct - 50) < 0.1:\n    print(\"Perfect balance: 50-50!\")\nelse:\n    print(f\" Balance: {class_0_pct:.2f}% - {class_1_pct:.2f}%\")\n\nprint()\nprint(\" TRAIN SET ƒê√É ƒê∆Ø·ª¢C C√ÇN B·∫∞NG!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================\n# V·∫º BI·ªÇU ƒê·ªí: TRAIN SET SAU KHI √ÅP D·ª§NG SMOTE\n# ==============================================================\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\n# T√≠nh t·ª∑ l·ªá % c·ªßa y_train_over (sau SMOTE)\nplot_dataseries = round(y_train_over.value_counts(normalize=True) * 100, 2)\n\n# V·∫Ω bi·ªÉu ƒë·ªì c·ªôt\nplot_dataseries.plot(kind=\"bar\", ax=ax, color=\"navy\")\n\n# Th√™m text x√°c nh·∫≠n c√¢n b·∫±ng\nplt.text(0.5, 70, \"Dataset L·ªõp C√¢n b·∫±ng (ƒê√£ Oversample)\", color=\"darkgreen\",\n         horizontalalignment=\"center\", fontsize=14, fontweight=700)\nplt.axhline(y=plot_dataseries[0], color=\"darkgreen\", linestyle=\"--\")\n\n# Ti√™u ƒë·ªÅ v√† nh√£n\nplt.title(\"T·ª∑ l·ªá c√°c L·ªõp c·ªßa y_train_over (Sau SMOTE)\", fontsize=16)\nplt.ylabel(\"T·ª∑ l·ªá (%)\", fontsize=12)\nplt.xlabel(\"Tr·∫°ng th√°i\", fontsize=12)\nplt.xticks(ticks=range(len(plot_dataseries)),\n           labels=[\"Kh√¥ng b·ªã t·ª´ ch·ªëi (0)\", \"B·ªã t·ª´ ch·ªëi (1)\"],\n           rotation=\"horizontal\")\nplt.yticks(ticks=[20, 40, 60, 80, 100], labels=[\"20%\", \"40%\", \"60%\", \"80%\", \"100%\"])\n\n# Th√™m nh√£n % l√™n c·ªôt\ndata_label = plot_dataseries.astype(str).str.cat(np.full((2,), \"%\"), sep=\"\")\nfor x, y in enumerate(plot_dataseries):\n    plt.text(x, y-10, data_label[x], color=\"white\",\n             fontweight=700, fontsize=13, horizontalalignment=\"center\")\n\n# Th√™m nh√£n s·ªë ƒë·∫øm\nfor x, y in enumerate(y_train_over.value_counts()):\n    plt.text(x, plot_dataseries[x]-25,\n             f\"S·ªë l∆∞·ª£ng:\\n{y_train_over.value_counts()[x]}\",\n             horizontalalignment=\"center\", color=\"lightgreen\", fontweight=700)\n\nplt.show()\n\nprint(\"\\n Bi·ªÉu ƒë·ªì ƒë√£ ho√†n th√†nh!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **K·∫æT QU·∫¢ SAU KHI √ÅP D·ª§NG SMOTE:**\n\n**Train set ƒë√£ ƒë∆∞·ª£c c√¢n b·∫±ng:**\n- **Tr∆∞·ªõc SMOTE:** ~58% class 0, ~42% class 1 (l·ªách)\n- **Sau SMOTE:** 50% class 0, 50% class 1 (c√¢n b·∫±ng ho√†n h·∫£o)\n\n**L·ª£i √≠ch:**\n1. **Gi·∫£m bias:** Model kh√¥ng b·ªã thi√™n v·ªã v·ªÅ class ƒëa s·ªë\n2. **C·∫£i thi·ªán Recall:** Model h·ªçc ƒë∆∞·ª£c minority class t·ªët h∆°n\n3. **Synthetic samples:** SMOTE t·∫°o samples m·ªõi (kh√¥ng duplicate) ‚Üí tr√°nh overfitting\n4. **Preserve test set:** Test set gi·ªØ nguy√™n distribution g·ªëc ‚Üí ƒë√°nh gi√° c√¥ng b·∫±ng\n","metadata":{}},{"cell_type":"markdown","source":"### **6.c. Data Transformation (Pipelines)**\n\n**M·ª•c ƒë√≠ch:** \n- X·ª≠ l√Ω NaN (n·∫øu c√≤n)\n- Scale numerical features\n- Encode categorical features\n\n**Ph∆∞∆°ng ph√°p: Sklearn Pipelines + ColumnTransformer**\n\nSklearn pipelines cho ph√©p g√≥i t·∫•t c·∫£ preprocessing steps v√†o m·ªôt object duy nh·∫•t, ƒë·∫£m b·∫£o c√πng transformations ƒë∆∞·ª£c √°p d·ª•ng ƒë·ªìng nh·∫•t cho train v√† test data (tr√°nh data leakage).  ColumnTransformer cho ph√©p √°p d·ª•ng transforms kh√°c nhau cho t·ª´ng lo·∫°i c·ªôt (numerical vs categorical).","metadata":{}},{"cell_type":"markdown","source":"### <br>**6.c.ii.** NUMERICAL PIPELINE.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# T·∫°o pipeline: ƒêi·ªÅn gi√° tr·ªã thi·∫øu b·∫±ng trung v·ªã -> Chu·∫©n h√≥a\nnum_transformer = make_pipeline(\n    SimpleImputer(strategy='median'),  # Trung v·ªã √≠t b·ªã ·∫£nh h∆∞·ªüng b·ªüi ngo·∫°i lai h∆°n trung b√¨nh\n    StandardScaler()  # C√¥ng th·ª©c: (x - trung b√¨nh) / ƒë·ªô l·ªách chu·∫©n\n)\n\nprint(\" Numerical transformer pipeline created:\")\nnum_transformer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <br>**6.c.ii.** CATEGORICAL PIPELINE.","metadata":{}},{"cell_type":"code","source":"# Pipeline: SimpleImputer (ƒëi·ªÅn NaN) ‚Üí OneHotEncoder (encode)\n# - SimpleImputer v·ªõi strategy='most_frequent': ƒêi·ªÅn b·∫±ng mode\n# - OneHotEncoder v·ªõi drop='first': Tr√°nh dummy variable trap\n\ncat_transformer = make_pipeline(\n    SimpleImputer(strategy=\"most_frequent\"),  # ƒêi·ªÅn NaN b·∫±ng mode\n    OneHotEncoder(drop=\"first\", handle_unknown='ignore')  # OneHot encode\n)\n\nprint(\" Categorical transformer created:\")\nprint(cat_transformer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <br>**6.c.iii.** iii. ColumnTransformer (B·ªô bi·∫øn ƒë·ªïi c·ªôt) ƒë·ªÉ ch·∫°y song song.\n","metadata":{}},{"cell_type":"code","source":"# C·∫≠p nh·∫≠t b·ªô bi·∫øn ƒë·ªïi d·ªØ li·ªáu ph√¢n lo·∫°i ƒë·ªÉ x·ª≠ l√Ω c√°c danh m·ª•c ch∆∞a xu·∫•t hi·ªán\ncat_transformer = make_pipeline(\n    SimpleImputer(strategy=\"most_frequent\"),\n    OneHotEncoder(drop=\"first\", handle_unknown='ignore')  # Th√™m tham s·ªë x·ª≠ l√Ω gi√° tr·ªã l·∫°\n)\n\n# Kh·ªüi t·∫°o ColumnTransformer\ncol_transformer = ColumnTransformer([\n    (\"numtransformer\", num_transformer,\n     X_train_over.select_dtypes(exclude=\"object\").columns),\n    (\"cattransformer\", cat_transformer,\n     X_train_over.select_dtypes(include=\"object\").columns)\n], remainder='drop')  # Lo·∫°i b·ªè c√°c c·ªôt c√≤n l·∫°i kh√¥ng ƒë∆∞·ª£c li·ªát k√™\n\nprint(\"Column transformer created!\")\nprint(f\"\\nNumerical features: {list(X_train_over.select_dtypes(exclude='object').columns)}\")\nprint(f\"Categorical features: {list(X_train_over.select_dtypes(include='object').columns)}\")\n\ncol_transformer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **7. X√ÇY D·ª∞NG V√Ä ƒê√ÅNH GI√Å M√î H√åNH MACHINE LEARNING**","metadata":{}},{"cell_type":"markdown","source":"## 7.1. IMPORT MODELS V√Ä T·∫†O DATAFRAME L∆ØU K·∫æT QU·∫¢","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, \n                             f1_score, roc_auc_score, roc_curve, \n                             confusion_matrix, ConfusionMatrixDisplay, \n                             classification_report)\n\n# T·∫°o dataframe l∆∞u k·∫øt qu·∫£ c√°c models\nresults_df = pd.DataFrame(columns=[\n    'Model', 'Baseline_Accuracy', 'Tuned_Accuracy', \n    'Precision', 'Recall', 'F1_Score', 'AUC_Score'\n])\n\nprint(\"Bat dau xay dung cac model Machine Learning...\")\nprint(\"DataFrame luu ket qua da duoc tao.\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7.2. Model: XGBoost\n","metadata":{}},{"cell_type":"markdown","source":"### B∆∞·ªõc 1: BASELINE (Default Parameters)","metadata":{}},{"cell_type":"code","source":"# ==============================================================\n# MODEL 4: XGBoost (EXTREME GRADIENT BOOSTING)\n# ==============================================================\n\nprint(\"=\"*70)\nprint(\"MODEL 4: XGBoost (EXTREME GRADIENT BOOSTING)\")\nprint(\"=\"*70)\n\n# ==============================================================\n# B∆Ø·ªöC 1: BASELINE (Default Parameters)\n# ==============================================================\n\nprint(\"\\nB∆Ø·ªöC 1: CH·∫†Y BASELINE (Default Hyperparameters)\")\nprint(\"-\"*70)\n\nfrom xgboost import XGBClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# T·∫°o baseline model v·ªõi default parameters\nxgb_baseline = XGBClassifier(\n    random_state=42,\n    eval_metric='logloss',\n    use_label_encoder=False\n)\n\n# T·∫°o pipeline: Transform ‚Üí Model\nxgb_baseline_pipeline = make_pipeline(col_transformer, xgb_baseline)\n\nprint(\"Default parameters:\")\nprint(\"  - n_estimators: 100 (s·ªë trees)\")\nprint(\"  - learning_rate: 0.3 (shrinkage factor)\")\nprint(\"  - max_depth: 6 (ƒë·ªô s√¢u m·ªói tree)\")\nprint(\"  - subsample: 1.0 (d√πng 100% samples)\")\nprint(\"  - colsample_bytree: 1.0 (d√πng 100% features)\")\n\nprint(\"\\nƒêang training baseline...\")\nxgb_baseline_pipeline.fit(X_train_over, y_train_over)\n\n# D·ª± ƒëo√°n tr√™n test set\ny_pred_baseline_xgb = xgb_baseline_pipeline.predict(X_test)\ny_pred_proba_baseline_xgb = xgb_baseline_pipeline.predict_proba(X_test)[:, 1]\n\n# T√≠nh metrics\nbaseline_accuracy_xgb = accuracy_score(y_test, y_pred_baseline_xgb)\nbaseline_f1_xgb = f1_score(y_test, y_pred_baseline_xgb)\nbaseline_auc_xgb = roc_auc_score(y_test, y_pred_proba_baseline_xgb)\n\nprint(f\"\\nBASELINE RESULTS:\")\nprint(f\"  Accuracy:  {baseline_accuracy_xgb:.4f} ({baseline_accuracy_xgb*100:.2f}%)\")\nprint(f\"  F1-Score:  {baseline_f1_xgb:.4f}\")\nprint(f\"  AUC Score: {baseline_auc_xgb:.4f}\")\n\nprint(\" Ho√†n th√†nh Baseline!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### B∆Ø·ªöC 2 - HYPERPARAMETER TUNING","metadata":{}},{"cell_type":"code","source":"# ==============================================================\n# B∆Ø·ªöC 2: HYPERPARAMETER TUNING\n# ==============================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"B∆Ø·ªöC 2: HYPERPARAMETER TUNING\")\nprint(\"=\"*70)\n\n# ƒê·ªãnh nghƒ©a hyperparameter grid\nparam_grid_xgb = {\n    'xgbclassifier__n_estimators': [50, 100, 150],\n    \n    # Learning rate\n    'xgbclassifier__learning_rate': [0.1, 0.15, 0.2],\n    \n    # ƒê·ªô s√¢u tree\n    'xgbclassifier__max_depth': [5, 7, 10],\n    \n    # Subsampling \n    'xgbclassifier__subsample': [0.8, 1.0],\n    \n    # Column sampling\n    'xgbclassifier__colsample_bytree': [0.8, 1.0],\n    \n    # Regularization \n    'xgbclassifier__gamma': [0, 0.1, 0.5],\n    \n    # Handle imbalance\n    'xgbclassifier__scale_pos_weight': [1, 1.38∆Ø]\n}\n\nprint(\"\\nHyperparameters ƒë·ªÉ tuning:\")\nprint(\"  1. n_estimators: [100, 200, 300]\")\nprint(\"     ‚Üí S·ªë trees trong ensemble\")\nprint(\"  2. learning_rate: [0.01, 0.05, 0.1, 0.3]\")\nprint(\"     ‚Üí Shrinkage factor (th·∫•p ‚Üí robust, c·∫ßn nhi·ªÅu trees)\")\nprint(\"  3. max_depth: [3, 5, 7]\")\nprint(\"     ‚Üí ƒê·ªô s√¢u m·ªói tree (s√¢u ‚Üí complex ‚Üí overfit)\")\nprint(\"  4.  subsample: [0.8, 1.0]\")\nprint(\"     ‚Üí T·ª∑ l·ªá samples m·ªói tree (0.8 ‚Üí stochastic ‚Üí reduce overfit)\")\nprint(\"  5. colsample_bytree: [0.8, 1.0]\")\nprint(\"     ‚Üí T·ª∑ l·ªá features m·ªói tree (0.8 ‚Üí diversity)\")\nprint(\"  6. gamma: [0, 0.1, 0.5]\")\nprint(\"     ‚Üí Minimum loss reduction ƒë·ªÉ split (l·ªõn ‚Üí simple)\")\nprint(\"  7. scale_pos_weight: [1, 1.38]\")\nprint(\"     ‚Üí Weight cho minority class\")\n\ntotal_combinations = 3 * 4 * 3 * 2 * 2 * 3 * 2\nprint(f\"\\nT·ªïng s·ªë combinations: {total_combinations}\")\nprint(f\"V·ªõi 5-fold CV: {total_combinations} x 5 = {total_combinations*5} fits\")\n\n# T·∫°o pipeline\nxgb_pipeline = make_pipeline(\n    col_transformer,\n    XGBClassifier(random_state=42, eval_metric='logloss', use_label_encoder=False)\n)\n\n# GridSearchCV\ngrid_search_xgb = GridSearchCV(\n    estimator=xgb_pipeline,\n    param_grid=param_grid_xgb,\n    cv=5,\n    scoring='f1',\n    n_jobs=-1,\n    verbose=1\n)\n\nprint(\"\\nƒêang ch·∫°y GridSearchCV...\")\nprint(\"(C√≥ th·ªÉ m·∫•t 15-30 ph√∫t - XGBoost t·ªën th·ªùi gian nh·∫•t! )\\n\")\n\ngrid_search_xgb.fit(X_train_over, y_train_over)\n\nprint(\"\\n GridSearchCV ho√†n th√†nh!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# In ra best parameters\nprint(\"\\nBEST HYPERPARAMETERS:\")\nprint(\"=\"*70)\n\nfor param, value in grid_search_xgb.best_params_.items():\n    clean_param = param.replace('xgbclassifier__', '')\n    print(f\"  {clean_param}: {value}\")\n\nprint(f\"\\nBest Cross-Validation F1-Score: {grid_search_xgb.best_score_:.4f}\")\n\n# L∆∞u best parameters\nbest_n_est_xgb = grid_search_xgb.best_params_['xgbclassifier__n_estimators']\nbest_lr_xgb = grid_search_xgb.best_params_['xgbclassifier__learning_rate']\nbest_depth_xgb = grid_search_xgb.best_params_['xgbclassifier__max_depth']\nbest_subsample_xgb = grid_search_xgb.best_params_['xgbclassifier__subsample']\nbest_colsample_xgb = grid_search_xgb.best_params_['xgbclassifier__colsample_bytree']\nbest_gamma_xgb = grid_search_xgb.best_params_['xgbclassifier__gamma']\nbest_scale_xgb = grid_search_xgb.best_params_['xgbclassifier__scale_pos_weight']\n\n# Gi·∫£i th√≠ch\nprint(\"\\nGi·∫£i th√≠ch Best Parameters:\")\nprint(f\"  ‚Ä¢ n_estimators = {best_n_est_xgb}:\")\nif best_n_est_xgb == 100:\n    print(f\"      ‚Üí Default ƒë√£ ƒë·ªß, kh√¥ng c·∫ßn th√™m trees\")\nelif best_n_est_xgb == 200:\n    print(f\"      ‚Üí Sweet spot - c√¢n b·∫±ng t·ªët\")\nelse:\n    print(f\"      ‚Üí C·∫ßn nhi·ªÅu trees - patterns ph·ª©c t·∫°p\")\n\nprint(f\"  ‚Ä¢ learning_rate = {best_lr_xgb}:\")\nif best_lr_xgb <= 0.05:\n    print(f\"      ‚Üí Slow learning - robust, c·∫ßn nhi·ªÅu trees\")\nelif best_lr_xgb == 0.1:\n    print(f\"      ‚Üí Balanced learning rate\")\nelse:\n    print(f\"      ‚Üí Fast learning - default works\")\n\nprint(f\"  ‚Ä¢ max_depth = {best_depth_xgb}:\")\nif best_depth_xgb <= 3:\n    print(f\"      ‚Üí Shallow trees - simple patterns\")\nelif best_depth_xgb <= 5:\n    print(f\"      ‚Üí Moderate depth - balanced\")\nelse:\n    print(f\"      ‚Üí Deep trees - complex patterns\")\n\nprint(f\"  ‚Ä¢ subsample = {best_subsample_xgb}:\")\nprint(f\"      ‚Üí {'Stochastic boosting' if best_subsample_xgb < 1.0 else 'D√πng h·∫øt samples'}\")\n\nprint(f\"  ‚Ä¢ colsample_bytree = {best_colsample_xgb}:\")\nprint(f\"      ‚Üí {'Feature subsampling' if best_colsample_xgb < 1.0 else 'D√πng h·∫øt features'}\")\n\nprint(f\"  ‚Ä¢ gamma = {best_gamma_xgb}:\")\nprint(f\"      ‚Üí {'Aggressive splitting' if best_gamma_xgb == 0 else 'Conservative splitting'}\")\n\nprint(f\"  ‚Ä¢ scale_pos_weight = {best_scale_xgb}:\")\nprint(f\"      ‚Üí {'SMOTE ƒë√£ ƒë·ªß' if best_scale_xgb == 1 else 'C·∫ßn weight th√™m cho minority class'}\")\n\n# L·∫•y best model\nxgb_best_model = grid_search_xgb.best_estimator_\nprint(\"\\n Best model ƒë√£ ƒë∆∞·ª£c l∆∞u!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T17:51:54.303486Z","iopub.execute_input":"2025-12-05T17:51:54.303820Z","iopub.status.idle":"2025-12-05T17:51:54.385784Z","shell.execute_reply.started":"2025-12-05T17:51:54.303797Z","shell.execute_reply":"2025-12-05T17:51:54.384511Z"}},"outputs":[{"name":"stdout","text":"\nBEST HYPERPARAMETERS:\n======================================================================\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_140/2825838683.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrid_search_xgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mclean_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xgbclassifier__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  {clean_param}: {value}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'grid_search_xgb' is not defined"],"ename":"NameError","evalue":"name 'grid_search_xgb' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"markdown","source":"### B∆Ø·ªöC 3: ƒê√ÅNH GI√Å CHI TI·∫æT MODEL","metadata":{}},{"cell_type":"code","source":"# ==============================================================\n# B∆Ø·ªöC 3: ƒê√ÅNH GI√Å CHI TI·∫æT MODEL\n# ==============================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"B∆Ø·ªöC 3: ƒê√ÅNH GI√Å CHI TI·∫æT MODEL\")\nprint(\"=\"*70)\n\n# D·ª± ƒëo√°n tr√™n test set v·ªõi best model\ny_pred_test_xgb = xgb_best_model.predict(X_test)\ny_pred_proba_test_xgb = xgb_best_model.predict_proba(X_test)[:, 1]\n\n# --------------------------------------------------------------\n# 3.1.  ACCURACY\n# --------------------------------------------------------------\nprint(\"\\n3.1. ACCURACY\")\nprint(\"-\"*70)\n\naccuracy_test_xgb = accuracy_score(y_test, y_pred_test_xgb)\nprint(f\"Test Accuracy (Tuned Model): {accuracy_test_xgb:.4f} ({accuracy_test_xgb*100:.2f}%)\")\nprint(f\"  ‚Üí Model d·ª± ƒëo√°n ƒê√öNG {accuracy_test_xgb*100:.1f}% tr√™n test set\")\n\n# So s√°nh ng·∫Øn g·ªçn v·ªõi Baseline\nimprovement_xgb = (accuracy_test_xgb - baseline_accuracy_xgb) * 100\nprint(f\"\\nSo s√°nh v·ªõi Baseline:\")\nprint(f\"  Baseline: {baseline_accuracy_xgb:.4f} ‚Üí Tuned: {accuracy_test_xgb:.4f} ({improvement_xgb:+.2f}%)\")\n\nif improvement_xgb > 0:\n    print(f\"  Tuning ƒë√£ c·∫£i thi·ªán XGBoost!\")\nelif improvement_xgb == 0:\n    print(f\"  ‚Üí Kh√¥ng thay ƒë·ªïi (default ƒë√£ optimal)\")\nelse:\n    print(f\"  ‚Üí Default ƒë√£ r·∫•t t·ªët, tuning gi·∫£m nh·∫π (c√≥ th·ªÉ overfit CV)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------------------------------------------------------------\n# 3.2.  CLASSIFICATION REPORT\n# --------------------------------------------------------------\nprint(\"\\n3.2.  CLASSIFICATION REPORT\")\nprint(\"-\"*70)\n\nprint(classification_report(y_test, y_pred_test_xgb,\n                           target_names=['Not Rejected (0)', 'Rejected (1)']))\n\n# T√≠nh metrics\nprecision_xgb = precision_score(y_test, y_pred_test_xgb)\nrecall_xgb = recall_score(y_test, y_pred_test_xgb)\nf1_xgb = f1_score(y_test, y_pred_test_xgb)\n\nprint(f\"\\nMetrics cho class 'Rejected' (Class 1):\")\nprint(f\"  Precision: {precision_xgb:.4f} ‚Üí {precision_xgb*100:.1f}% d·ª± ƒëo√°n 'Rejected' ch√≠nh x√°c\")\nprint(f\"  Recall:    {recall_xgb:.4f} ‚Üí Ph√°t hi·ªán {recall_xgb*100:.1f}% ƒë∆°n th·ª±c s·ª± b·ªã rejected\")\nprint(f\"  F1-Score:  {f1_xgb:.4f} ‚Üí C√¢n b·∫±ng gi·ªØa Precision v√† Recall\")\n\n# Nh·∫≠n x√©t\nprint(\"\\nNh·∫≠n x√©t:\")\nif f1_xgb >= 0.85:\n    print(f\"  ‚Ä¢ F1={f1_xgb:.2f}: EXCELLENT!  XGBoost balance t·ªët\")\nelif f1_xgb >= 0.75:\n    print(f\"  ‚Ä¢ F1={f1_xgb:.2f}: GOOD - Performance t·ªët\")\nelse:\n    print(f\"  ‚Ä¢ F1={f1_xgb:.2f}: C√≥ th·ªÉ c·∫£i thi·ªán\")\n\nif recall_xgb >= 0.85:\n    print(f\"  ‚Ä¢ Recall cao ({recall_xgb:.2f}): Ph√°t hi·ªán t·ªët c√°c ƒë∆°n rejected\")\nelif recall_xgb >= 0.75:\n    print(f\"  ‚Ä¢ Recall trung b√¨nh ({recall_xgb:.2f}): Ch·∫•p nh·∫≠n ƒë∆∞·ª£c\")\nelse:\n    print(f\"  ‚Ä¢ Recall th·∫•p ({recall_xgb:.2f}): B·ªè s√≥t nhi·ªÅu\")\n\nif precision_xgb >= 0.85:\n    print(f\"  ‚Ä¢ Precision cao ({precision_xgb:.2f}): √çt false alarms\")\nelif precision_xgb >= 0.75:\n    print(f\"  ‚Ä¢ Precision trung b√¨nh ({precision_xgb:.2f})\")\nelse:\n    print(f\"  ‚Ä¢ Precision th·∫•p ({precision_xgb:.2f}): Nhi·ªÅu false alarms\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------------------------------------------------------------\n# 3.3.  CONFUSION MATRIX\n# --------------------------------------------------------------\nprint(\"\\n3.3.  CONFUSION MATRIX\")\nprint(\"-\"*70)\n\ncm_xgb = confusion_matrix(y_test, y_pred_test_xgb)\n\n# V·∫Ω confusion matrix\nfig, ax = plt.subplots(figsize=(8, 6))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_xgb,\n                               display_labels=['Not Rejected', 'Rejected'])\ndisp.plot(cmap='Oranges', ax=ax, values_format='d')\nplt.title(f'XGBoost - Confusion Matrix\\n(n_estimators={best_n_est_xgb}, learning_rate={best_lr_xgb}, max_depth={best_depth_xgb})',\n          fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Ph√¢n t√≠ch\ntn_xgb, fp_xgb, fn_xgb, tp_xgb = cm_xgb.ravel()\nprint(f\"\\nChi ti·∫øt Confusion Matrix:\")\nprint(f\"  True Negatives (TN):  {tn_xgb:4d} ‚Üí D·ª± ƒëo√°n ƒë√∫ng 'Not Rejected'\")\nprint(f\"  False Positives (FP): {fp_xgb:4d} ‚Üí Nh·∫ßm: d·ª± ƒëo√°n 'Rejected' nh∆∞ng th·ª±c t·∫ø 'Not Rejected'\")\nprint(f\"  False Negatives (FN): {fn_xgb:4d} ‚Üí Nh·∫ßm: d·ª± ƒëo√°n 'Not Rejected' nh∆∞ng th·ª±c t·∫ø 'Rejected'\")\nprint(f\"  True Positives (TP):  {tp_xgb:4d} ‚Üí D·ª± ƒëo√°n ƒë√∫ng 'Rejected'\")\n\ntotal_errors = fp_xgb + fn_xgb\nprint(f\"\\nT·ªïng l·ªói: {total_errors} / {len(y_test)} = {total_errors/len(y_test)*100:.1f}%\")\n\n# Ph√¢n t√≠ch pattern l·ªói\nif fn_xgb > fp_xgb:\n    print(\"  ‚Üí False Negatives > False Positives\")\n    print(\"  ‚Üí Model thi√™n v·ªÅ d·ª± ƒëo√°n 'Not Rejected' (conservative)\")\nelse:\n    print(\"  ‚Üí False Positives > False Negatives\")\n    print(\"  ‚Üí Model thi√™n v·ªÅ d·ª± ƒëo√°n 'Rejected' (aggressive)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------------------------------------------------------------\n# 3.4.  ROC CURVE & AUC SCORE\n# --------------------------------------------------------------\nprint(\"\\n3.4. ROC CURVE & AUC SCORE\")\nprint(\"-\"*70)\n\n# T√≠nh ROC curve\nfpr_xgb, tpr_xgb, thresholds_xgb = roc_curve(y_test, y_pred_proba_test_xgb)\nauc_score_xgb = roc_auc_score(y_test, y_pred_proba_test_xgb)\n\n# V·∫Ω ROC curve\nfig, ax = plt.subplots(figsize=(8, 6))\nax.plot(fpr_xgb, tpr_xgb, color='darkorange', lw=2,\n        label=f'XGBoost (AUC = {auc_score_xgb:.4f})')\nax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--',\n        label='Random Classifier (AUC = 0.5)')\nax.set_xlim([0.0, 1.0])\nax.set_ylim([0.0, 1.05])\nax.set_xlabel('False Positive Rate (FPR)', fontsize=12)\nax.set_ylabel('True Positive Rate (TPR)', fontsize=12)\nax.set_title(f'XGBoost - ROC Curve\\n(n_estimators={best_n_est_xgb}, learning_rate={best_lr_xgb})',\n             fontsize=14, fontweight='bold')\nax.legend(loc=\"lower right\", fontsize=11)\nax.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nAUC Score: {auc_score_xgb:.4f}\")\n\n# ƒê√°nh gi√° AUC\nif auc_score_xgb >= 0.9:\n    print(f\"  ‚Üí OUTSTANDING!  Model c√≥ discriminative power r·∫•t t·ªët\")\nelif auc_score_xgb >= 0.8:\n    print(f\"  ‚Üí EXCELLENT! Model ph√¢n bi·ªát 2 classes t·ªët\")\nelif auc_score_xgb >= 0.7:\n    print(f\"  ‚Üí ACCEPTABLE, nh∆∞ng c√≥ th·ªÉ c·∫£i thi·ªán\")\nelse:\n    print(f\"  ‚Üí POOR, c·∫ßn xem x√©t l·∫°i model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------------------------------------------------------------\n# 3.5.  FEATURE IMPORTANCE ‚≠ê\n# --------------------------------------------------------------\nprint(\"\\n3.5. FEATURE IMPORTANCE\")\nprint(\"-\"*70)\n\n# Extract XGBoost model t·ª´ pipeline\nxgb_model = xgb_best_model.named_steps['xgbclassifier']\n\n# Get feature names sau transform\nfeature_names = xgb_best_model.named_steps['columntransformer'].get_feature_names_out()\n\n# Get feature importance (default = 'gain')\nimportance_gain = xgb_model.feature_importances_\n\n# T·∫°o DataFrame\nimportance_df = pd.DataFrame({\n    'Feature': feature_names,\n    'Importance': importance_gain\n}).sort_values('Importance', ascending=False)\n\n# Normalize to percentage\nimportance_df['Importance_Pct'] = (importance_df['Importance'] / importance_df['Importance'].sum()) * 100\n\nprint(\"\\nTop 15 Features quan tr·ªçng nh·∫•t:\\n\")\nprint(importance_df.head(15).to_string(index=False))\n\n# Plot Feature Importance\nfig, ax = plt.subplots(figsize=(10, 8))\ntop_15 = importance_df.head(15)\nbars = ax.barh(range(len(top_15)), top_15['Importance'], color='darkorange')\n\n# Highlight top 5\nfor i in range(min(5, len(bars))):\n    bars[i].set_color('darkred')\n\nax.set_yticks(range(len(top_15)))\nax.set_yticklabels(top_15['Feature'], fontsize=10)\nax.set_xlabel('Importance (Gain)', fontsize=12)\nax.set_title('XGBoost - Top 15 Feature Importance\\n(Top 5 in Dark Red)',\n             fontsize=14, fontweight='bold')\nax.invert_yaxis()\n\n# Th√™m percentage labels\nfor i, (idx, row) in enumerate(top_15.iterrows()):\n    ax.text(row['Importance'], i, f\"  {row['Importance_Pct']:.1f}%\",\n            va='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nüí° Gi·∫£i th√≠ch:\")\nprint(\"  ‚Ä¢ Importance = Gain (improvement in loss khi split)\")\nprint(\"  ‚Ä¢ Features c√†ng cao c√†ng quan tr·ªçng cho predictions\")\nprint(\"  ‚Ä¢ Business n√™n t·∫≠p trung v√†o top features ƒë·ªÉ c·∫£i thi·ªán\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------------------------------------------------------------\n# 3.6. L∆ØU K·∫æT QU·∫¢ V√ÄO B·∫¢NG T·ªîNG H·ª¢P\n# --------------------------------------------------------------\nprint(\"\\n3.6. L∆ØU K·∫æT QU·∫¢ V√ÄO B·∫¢NG T·ªîNG H·ª¢P\")\nprint(\"-\"*70)\n\n# Th√™m k·∫øt qu·∫£ XGBoost v√†o dataframe\nresults_df = pd.concat([results_df, pd.DataFrame([{\n    'Model': 'XGBoost',\n    'Baseline_Accuracy': baseline_accuracy_xgb,\n    'Tuned_Accuracy': accuracy_test_xgb,\n    'Precision': precision_xgb,\n    'Recall': recall_xgb,\n    'F1_Score': f1_xgb,\n    'AUC_Score': auc_score_xgb\n}])], ignore_index=True)\n\nprint(\" K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u!\")\nprint(\"\\nB·∫£ng t·ªïng h·ª£p hi·ªán t·∫°i:\")\nprint(results_df.to_string(index=False))\n\n# T√≥m t·∫Øt\nprint(\"\\n\" + \"=\"*70)\nprint(\"XGBoost - HO√ÄN TH√ÄNH!\")\nprint(\"=\"*70)\nprint(f\"\\nK·∫øt qu·∫£ cu·ªëi c√πng:\")\nprint(f\"  ‚Ä¢ Best n_estimators: {best_n_est_xgb}\")\nprint(f\"  ‚Ä¢ Best learning_rate: {best_lr_xgb}\")\nprint(f\"  ‚Ä¢ Best max_depth: {best_depth_xgb}\")\nprint(f\"  ‚Ä¢ Final Accuracy: {accuracy_test_xgb:.4f} ({accuracy_test_xgb*100:.2f}%)\")\nprint(f\"  ‚Ä¢ F1-Score: {f1_xgb:.4f}\")\nprint(f\"  ‚Ä¢ AUC Score: {auc_score_xgb:.4f}\")\nprint(f\"  ‚Ä¢ Training time: ~15-30 ph√∫t (864 combinations! )\")\n\nprint(\"\\nModel XGBoost ƒë√£ s·∫µn s√†ng!\")\n\n# ============================================================\n# L∆ØU MODEL & K·∫æT QU·∫¢\n# ============================================================\nimport joblib\n\n# L∆∞u model\njoblib.dump(xgb_best_model, 'xgboost_tuned_pipeline.pkl')\nprint(\" ƒê√£ l∆∞u XGBoost model v√†o 'xgboost_tuned_pipeline.pkl'\")\n\n# L∆∞u k·∫øt qu·∫£ CSV\nresults_df.to_csv('results_xgboost.csv', index=False)\nprint(\" ƒê√£ l∆∞u k·∫øt qu·∫£ ra 'results_xgboost.csv'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}